{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a104b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hopsworks\n",
    "\n",
    "# Option to display all columns during debugging\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5d089d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Found 17 files.\n",
      " Base dataframe created. Shape: (26068, 9)\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Raw Files\n",
    "paths = sorted(glob.glob(\"data/raw/tomtom_snapshots_*.jsonl\"))\n",
    "print(f\" Found {len(paths)} files.\")\n",
    "\n",
    "rows = []\n",
    "for p in paths:\n",
    "    with open(p, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Loading line by line is robust for JSONL\n",
    "            rows.append(json.loads(line))\n",
    "\n",
    "df_raw = pd.DataFrame(rows)\n",
    "\n",
    "# 2. Function to extract only useful features and rename to snake_case\n",
    "def extract_features(payload):\n",
    "    if not isinstance(payload, dict): \n",
    "        return {}\n",
    "    \n",
    "    data = payload.get(\"flowSegmentData\", {})\n",
    "    if not data: \n",
    "        return {}\n",
    "    \n",
    "    return {\n",
    "        \"frc\": data.get(\"frc\"),\n",
    "        \"current_speed\": data.get(\"currentSpeed\"),\n",
    "        \"free_flow_speed\": data.get(\"freeFlowSpeed\"),\n",
    "        \"current_travel_time\": data.get(\"currentTravelTime\"),\n",
    "        \"free_flow_travel_time\": data.get(\"freeFlowTravelTime\"),\n",
    "        \"confidence\": data.get(\"confidence\"),\n",
    "        \"road_closure\": data.get(\"roadClosure\"),\n",
    "        # Removed \"coordinates\", \"version\", and \"source\" as they are not needed for the ML model\n",
    "    }\n",
    "\n",
    "# Apply extraction\n",
    "features_df = df_raw[\"payload\"].apply(extract_features).apply(pd.Series)\n",
    "\n",
    "# Join with essential base columns\n",
    "# We drop \"source\", \"version\", and \"payload\" here to keep it clean\n",
    "df = df_raw[[\"timestamp_utc\", \"point_id\", \"error\"]].join(features_df)\n",
    "\n",
    "# Filter out rows where extraction failed (error is not null)\n",
    "df = df[df[\"error\"].isna()].drop(columns=[\"error\"])\n",
    "\n",
    "print(f\" Base dataframe created. Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "de682976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataframe ready for upload. Unique rows: 26018\n",
      "Data Types:\n",
      " timestamp_utc            datetime64[ns, UTC]\n",
      "point_id                              object\n",
      "frc                                   object\n",
      "current_speed                        float64\n",
      "free_flow_speed                      float64\n",
      "current_travel_time                  float64\n",
      "free_flow_travel_time                float64\n",
      "confidence                           float64\n",
      "road_closure                            bool\n",
      "ts_10m                   datetime64[ns, UTC]\n",
      "speed_ratio                          float64\n",
      "delay_seconds                        float64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n"
     ]
    }
   ],
   "source": [
    "# 1. Handle Timestamps (UTC & 10-minute buckets)\n",
    "df[\"timestamp_utc\"] = pd.to_datetime(df[\"timestamp_utc\"], utc=True)\n",
    "df[\"ts_10m\"] = df[\"timestamp_utc\"].dt.floor(\"10min\")\n",
    "\n",
    "# 2. Type Casting\n",
    "# Ensure point_id is string for consistency\n",
    "df[\"point_id\"] = df[\"point_id\"].astype(str)\n",
    "\n",
    "# Cast numeric columns\n",
    "num_cols = [\"current_speed\", \"free_flow_speed\", \"current_travel_time\", \"free_flow_travel_time\", \"confidence\"]\n",
    "for c in num_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# --- CRITICAL FIX FOR HOPSWORKS/AVRO ---\n",
    "# Hopsworks Avro serializer often fails with Pandas nullable \"boolean\" (BooleanDtype).\n",
    "# We fill NaNs with False and convert to standard numpy 'bool'.\n",
    "df[\"road_closure\"] = df[\"road_closure\"].fillna(False).astype(bool)\n",
    "\n",
    "# 3. Feature Engineering\n",
    "# Calculate ratios and replace infinites with NaN\n",
    "df[\"speed_ratio\"] = df[\"current_speed\"] / df[\"free_flow_speed\"]\n",
    "df[\"delay_seconds\"] = df[\"current_travel_time\"] - df[\"free_flow_travel_time\"]\n",
    "\n",
    "df[\"speed_ratio\"] = df[\"speed_ratio\"].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# 4. Deduplication\n",
    "# Sort by ID, Time, and Confidence (descending) to keep the highest quality record per bucket\n",
    "df = df.sort_values(\n",
    "    by=[\"point_id\", \"ts_10m\", \"confidence\", \"timestamp_utc\"],\n",
    "    ascending=[True, True, False, False]\n",
    ")\n",
    "\n",
    "# Drop duplicates based on the primary key (Segment ID + Time Bucket)\n",
    "df = df.drop_duplicates(subset=[\"point_id\", \"ts_10m\"], keep=\"first\")\n",
    "\n",
    "print(f\" Dataframe ready for upload. Unique rows: {len(df)}\")\n",
    "print(\"Data Types:\\n\", df.dtypes) \n",
    "# Verify 'road_closure' is 'bool' (numpy) and NOT 'boolean' (pandas nullable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "af97e086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 11:41:33,771 INFO: Closing external client and cleaning up certificates.\n",
      "2026-01-11 11:41:33,784 INFO: Connection closed.\n",
      "2026-01-11 11:41:33,786 INFO: Initializing external client\n",
      "2026-01-11 11:41:33,786 INFO: Base URL: https://eu-west.cloud.hopsworks.ai:443\n",
      "2026-01-11 11:41:34,955 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://eu-west.cloud.hopsworks.ai:443/p/3209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 26018/26018 | Elapsed Time: 00:02 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: traffic_flow_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://eu-west.cloud.hopsworks.ai:443/p/3209/jobs/named/traffic_flow_fg_1_offline_fg_materialization/executions\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Job('traffic_flow_fg_1_offline_fg_materialization', 'SPARK'), None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Login to Hopsworks\n",
    "project = hopsworks.login(\n",
    "    host=\"eu-west.cloud.hopsworks.ai\",\n",
    "    project=\"London_traffic\"\n",
    ")\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "# 2. Get or Create the Feature Group\n",
    "# Using snake_case for the name is better practice\n",
    "traffic_fg = fs.get_or_create_feature_group(\n",
    "    name=\"traffic_flow_fg\",\n",
    "    version=1,\n",
    "    primary_key=[\"point_id\", \"ts_10m\"],\n",
    "    event_time=\"ts_10m\",\n",
    "    description=\"Traffic flow data aggregated by 10min buckets\",\n",
    ")\n",
    "\n",
    "# 3. Insert Data\n",
    "# wait_for_job=True ensures the cell waits until ingestion finishes (good for production/debugging)\n",
    "traffic_fg.insert(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e1c81ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df to csv\n",
    "df.to_csv(\"data/processed/traffic_flow_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
