{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea41415b",
   "metadata": {},
   "source": [
    "# 01 — Data collection & quick EDA\n",
    "\n",
    "Day 2–3 notebook.\n",
    "\n",
    "Prereqs:\n",
    "1) Create `.env` from `.env.example`\n",
    "2) Fill API keys\n",
    "3) Install requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51e858f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /Users/davidbazalduamendez/Documents/GitHub/Short-term-traffic-congestion-prediction-for-London-Final-Project\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "here = Path().resolve()\n",
    "if (here / \"src\").exists():\n",
    "    repo_root = here\n",
    "elif (here.parent / \"src\").exists():\n",
    "    repo_root = here.parent\n",
    "else:\n",
    "    repo_root = here.parents[1]\n",
    "\n",
    "sys.path.insert(0, str((repo_root / \"src\").resolve()))\n",
    "print(\"Repo root:\", repo_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad8940f",
   "metadata": {},
   "source": [
    "## Load clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0397fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic_forecast.config import get_settings\n",
    "from traffic_forecast.http import HttpClient, build_session\n",
    "from traffic_forecast.clients.tomtom import TomTomClient\n",
    "from traffic_forecast.clients.tfl import TflClient\n",
    "from traffic_forecast.clients.dft import DftRoadTrafficClient\n",
    "\n",
    "s = get_settings()\n",
    "http = HttpClient(session=build_session(user_agent=s.user_agent), timeout_seconds=s.http_timeout_seconds)\n",
    "tomtom = TomTomClient(api_key=s.tomtom_api_key, http=http)\n",
    "tfl = TflClient(app_key=s.tfl_app_key, app_id=s.tfl_app_id, http=http)\n",
    "dft = DftRoadTrafficClient(http=http)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae9b142",
   "metadata": {},
   "source": [
    "## Create/load monitoring points (DfT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a684a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local authorities fetched: 214\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>region_id</th>\n",
       "      <th>ita_id</th>\n",
       "      <th>ons_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Isles of Scilly</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>E06000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Nottinghamshire</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>E10000024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Glasgow City</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>S12000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>North Lanarkshire</td>\n",
       "      <td>3</td>\n",
       "      <td>None</td>\n",
       "      <td>S12000050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Somerset</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>E06000066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id               name  region_id ita_id   ons_code\n",
       "0   1    Isles of Scilly          1   None  E06000053\n",
       "1   2    Nottinghamshire          2   None  E10000024\n",
       "2   3       Glasgow City          3   None  S12000049\n",
       "3   4  North Lanarkshire          3   None  S12000050\n",
       "4   5           Somerset          1   None  E06000066"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greater London local authorities found: 33\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>ons_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>168</td>\n",
       "      <td>Barking and Dagenham</td>\n",
       "      <td>E09000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>Barnet</td>\n",
       "      <td>E09000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>106</td>\n",
       "      <td>Bexley</td>\n",
       "      <td>E09000004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>118</td>\n",
       "      <td>Brent</td>\n",
       "      <td>E09000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>176</td>\n",
       "      <td>Bromley</td>\n",
       "      <td>E09000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>145</td>\n",
       "      <td>Camden</td>\n",
       "      <td>E09000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>174</td>\n",
       "      <td>City of London</td>\n",
       "      <td>E09000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>134</td>\n",
       "      <td>Croydon</td>\n",
       "      <td>E09000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>147</td>\n",
       "      <td>Ealing</td>\n",
       "      <td>E09000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>121</td>\n",
       "      <td>Enfield</td>\n",
       "      <td>E09000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>105</td>\n",
       "      <td>Greenwich</td>\n",
       "      <td>E09000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>122</td>\n",
       "      <td>Hackney</td>\n",
       "      <td>E09000012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>146</td>\n",
       "      <td>Hammersmith and Fulham</td>\n",
       "      <td>E09000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>165</td>\n",
       "      <td>Haringey</td>\n",
       "      <td>E09000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>185</td>\n",
       "      <td>Harrow</td>\n",
       "      <td>E09000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                    name   ons_code\n",
       "167  168    Barking and Dagenham  E09000002\n",
       "56    57                  Barnet  E09000003\n",
       "105  106                  Bexley  E09000004\n",
       "117  118                   Brent  E09000005\n",
       "175  176                 Bromley  E09000006\n",
       "144  145                  Camden  E09000007\n",
       "173  174          City of London  E09000001\n",
       "133  134                 Croydon  E09000008\n",
       "146  147                  Ealing  E09000009\n",
       "120  121                 Enfield  E09000010\n",
       "104  105               Greenwich  E09000011\n",
       "121  122                 Hackney  E09000012\n",
       "145  146  Hammersmith and Fulham  E09000013\n",
       "164  165                Haringey  E09000014\n",
       "184  185                  Harrow  E09000015"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London count points collected (pre-clean): 1093\n",
      "Saved → /Users/davidbazalduamendez/Documents/GitHub/Short-term-traffic-congestion-prediction-for-London-Final-Project/notebooks/data/interim/monitoring_points.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>point_id</th>\n",
       "      <th>aadf_year</th>\n",
       "      <th>region_id</th>\n",
       "      <th>local_authority_id</th>\n",
       "      <th>road_name</th>\n",
       "      <th>road_category</th>\n",
       "      <th>road_type</th>\n",
       "      <th>start_junction_road_name</th>\n",
       "      <th>end_junction_road_name</th>\n",
       "      <th>easting</th>\n",
       "      <th>northing</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>link_length_km</th>\n",
       "      <th>link_length_miles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18526</td>\n",
       "      <td>18526</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>168</td>\n",
       "      <td>A13</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>LA boundary</td>\n",
       "      <td>A123/A1153</td>\n",
       "      <td>545000</td>\n",
       "      <td>183238</td>\n",
       "      <td>51.529441</td>\n",
       "      <td>0.088977</td>\n",
       "      <td>2.10</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7518</td>\n",
       "      <td>7518</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>168</td>\n",
       "      <td>A1240</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>A1306</td>\n",
       "      <td>A124</td>\n",
       "      <td>549040</td>\n",
       "      <td>185000</td>\n",
       "      <td>51.544222</td>\n",
       "      <td>0.147921</td>\n",
       "      <td>2.80</td>\n",
       "      <td>1.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6797</td>\n",
       "      <td>6797</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>176</td>\n",
       "      <td>A232</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>A233</td>\n",
       "      <td>A21</td>\n",
       "      <td>542500</td>\n",
       "      <td>165130</td>\n",
       "      <td>51.367362</td>\n",
       "      <td>0.045676</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36807</td>\n",
       "      <td>36807</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>105</td>\n",
       "      <td>A209</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>A206</td>\n",
       "      <td>LA Boundary</td>\n",
       "      <td>546030</td>\n",
       "      <td>178000</td>\n",
       "      <td>51.482110</td>\n",
       "      <td>0.101649</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28510</td>\n",
       "      <td>28510</td>\n",
       "      <td>2024</td>\n",
       "      <td>6</td>\n",
       "      <td>103</td>\n",
       "      <td>A200</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>A200 Jamaica St</td>\n",
       "      <td>A100</td>\n",
       "      <td>533653</td>\n",
       "      <td>179808</td>\n",
       "      <td>51.501415</td>\n",
       "      <td>-0.075800</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  point_id  aadf_year  region_id  local_authority_id road_name  \\\n",
       "0  18526     18526       2024          6                 168       A13   \n",
       "1   7518      7518       2024          6                 168     A1240   \n",
       "2   6797      6797       2024          6                 176      A232   \n",
       "3  36807     36807       2024          6                 105      A209   \n",
       "4  28510     28510       2024          6                 103      A200   \n",
       "\n",
       "  road_category road_type start_junction_road_name end_junction_road_name  \\\n",
       "0            PA     Major              LA boundary             A123/A1153   \n",
       "1            PA     Major                    A1306                   A124   \n",
       "2            PA     Major                     A233                    A21   \n",
       "3            PA     Major                     A206            LA Boundary   \n",
       "4            PA     Major          A200 Jamaica St                   A100   \n",
       "\n",
       "   easting  northing   latitude  longitude link_length_km link_length_miles  \n",
       "0   545000    183238  51.529441   0.088977           2.10              1.30  \n",
       "1   549040    185000  51.544222   0.147921           2.80              1.74  \n",
       "2   542500    165130  51.367362   0.045676           1.00              0.62  \n",
       "3   546030    178000  51.482110   0.101649           1.60              0.99  \n",
       "4   533653    179808  51.501415  -0.075800           0.30              0.19  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_PATH = Path(\"data/interim/monitoring_points.csv\")\n",
    "MAX_POINTS = 200\n",
    "LAS_PAGE_SIZE = 200\n",
    "COUNTPOINTS_PAGE_SIZE = 1000\n",
    "MAX_PAGES_COUNTPOINTS = 400  # safety para no irte al infinito\n",
    "\n",
    "def extract_data(resp):\n",
    "    # resp puede ser list o dict (paginado)\n",
    "    if resp is None:\n",
    "        return []\n",
    "    if isinstance(resp, list):\n",
    "        return resp\n",
    "    if isinstance(resp, dict):\n",
    "        return resp.get(\"data\", [])\n",
    "    return []\n",
    "\n",
    "def has_more_pages(resp, page):\n",
    "    # si viene dict con last_page, úsalo; si no (list), asumimos que no hay paginación visible\n",
    "    if isinstance(resp, dict):\n",
    "        last_page = resp.get(\"last_page\", page)\n",
    "        try:\n",
    "            return page < int(last_page)\n",
    "        except Exception:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load ALL local authorities (sin name_filter)\n",
    "# -------------------------\n",
    "all_las = []\n",
    "page = 1\n",
    "\n",
    "while True:\n",
    "    resp = dft.list_local_authorities(page_size=LAS_PAGE_SIZE, page_number=page, name_filter=None)\n",
    "    data = extract_data(resp)\n",
    "    if not data:\n",
    "        break\n",
    "    all_las.extend(data)\n",
    "\n",
    "    # si el endpoint expone paginación, seguimos; si no, salimos después de 1 página\n",
    "    if not has_more_pages(resp, page):\n",
    "        break\n",
    "    page += 1\n",
    "\n",
    "las_df = pd.DataFrame(all_las)\n",
    "print(\"local authorities fetched:\", len(las_df))\n",
    "display(las_df.head(5))\n",
    "\n",
    "# -------------------------\n",
    "# 2) Filter Greater London using ONS code (E09...)\n",
    "# -------------------------\n",
    "# en tu JSON viene ons_code por autoridad \n",
    "if \"ons_code\" not in las_df.columns:\n",
    "    raise RuntimeError(\"No encuentro la columna 'ons_code' en local authorities. Imprime las_df.columns para ver los nombres reales.\")\n",
    "\n",
    "london_las = las_df[\n",
    "    las_df[\"ons_code\"].astype(str).str.startswith(\"E09\", na=False)\n",
    "].copy()\n",
    "\n",
    "print(\"Greater London local authorities found:\", len(london_las))\n",
    "display(london_las[[\"id\", \"name\", \"ons_code\"]].sort_values(\"name\").head(15))\n",
    "\n",
    "london_la_ids = set(london_las[\"id\"].astype(int).tolist())\n",
    "if not london_la_ids:\n",
    "    raise RuntimeError(\"No pude identificar boroughs de Londres. Revisa si ons_code trae valores tipo E090000xx.\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Stream count points pages + filter by local_authority_id\n",
    "# -------------------------\n",
    "london_points = []\n",
    "page = 1\n",
    "\n",
    "while page <= MAX_PAGES_COUNTPOINTS and len(london_points) < MAX_POINTS * 5:\n",
    "    resp = dft.list_count_points(page_size=COUNTPOINTS_PAGE_SIZE, page_number=page)\n",
    "    data = extract_data(resp)\n",
    "    if not data:\n",
    "        break\n",
    "\n",
    "    for row in data:\n",
    "        la_id = row.get(\"local_authority_id\")\n",
    "        if la_id is None:\n",
    "            continue\n",
    "        try:\n",
    "            if int(la_id) in london_la_ids:\n",
    "                london_points.append(row)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if not has_more_pages(resp, page):\n",
    "        # si no hay paginación visible, ya no tiene sentido seguir\n",
    "        break\n",
    "    page += 1\n",
    "\n",
    "points_df = pd.DataFrame(london_points)\n",
    "print(\"London count points collected (pre-clean):\", len(points_df))\n",
    "\n",
    "# -------------------------\n",
    "# 4) Clean + sample + save\n",
    "# -------------------------\n",
    "if points_df.empty:\n",
    "    raise RuntimeError(\n",
    "        \"No se recolectaron count points de Londres en las páginas revisadas. \"\n",
    "        \"Sube MAX_PAGES_COUNTPOINTS o baja COUNTPOINTS_PAGE_SIZE si el API limita.\"\n",
    "    )\n",
    "\n",
    "points_df = (\n",
    "    points_df\n",
    "    .dropna(subset=[\"latitude\", \"longitude\"])\n",
    "    .drop_duplicates(subset=[\"count_point_id\"])\n",
    "    .rename(columns={\"count_point_id\": \"point_id\"})\n",
    ")\n",
    "\n",
    "points_df[\"latitude\"] = points_df[\"latitude\"].astype(float)\n",
    "points_df[\"longitude\"] = points_df[\"longitude\"].astype(float)\n",
    "\n",
    "points_df = points_df.sample(n=min(MAX_POINTS, len(points_df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "OUT_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "points_df[[\"point_id\", \"latitude\", \"longitude\"]].to_csv(OUT_PATH, index=False)\n",
    "\n",
    "print(f\"Saved → {OUT_PATH.resolve()}\")\n",
    "display(points_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a36757d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad5712f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "puntos: 50\n",
      "intervalo (min): 10\n",
      "duración (h): 8\n",
      "llamadas esperadas en esta corrida: 2400\n",
      "llamadas esperadas por día con este setup: 7200 (budget: 2500 )\n",
      "ojo: con este setup te pasas del budget diario, baja MAX_POINTS o sube INTERVAL_MIN\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "TOMTOM_API_KEY = os.getenv(\"TOMTOM_API_KEY\")\n",
    "if not TOMTOM_API_KEY:\n",
    "    raise RuntimeError(\"No encontré TOMTOM_API_KEY en tu .env\")\n",
    "\n",
    "POINTS_PATH = Path(\"data/interim/monitoring_points.csv\")  # ajusta si lo tienes en otra carpeta\n",
    "points = pd.read_csv(POINTS_PATH)\n",
    "\n",
    "required = {\"point_id\", \"latitude\", \"longitude\"}\n",
    "missing = required - set(points.columns)\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Faltan columnas en monitoring_points.csv: {missing}\")\n",
    "\n",
    "# ====== CONFIG CUOTA ======\n",
    "TOMTOM_DAILY_BUDGET = 2500      # non-tile requests/day :contentReference[oaicite:2]{index=2}\n",
    "INTERVAL_MIN = 10              # 30 min para no pasarte\n",
    "MAX_POINTS = 50                # 50 puntos → 2400 requests/día aprox\n",
    "DURATION_HOURS = 8             # histórico corto pero suficiente para baseline\n",
    "\n",
    "points = points.head(MAX_POINTS).copy()\n",
    "\n",
    "calls_per_hour = len(points) * (60 / INTERVAL_MIN)\n",
    "expected_calls = int(calls_per_hour * DURATION_HOURS)\n",
    "expected_per_day = int(len(points) * (60 / INTERVAL_MIN) * 24)\n",
    "\n",
    "print(\"puntos:\", len(points))\n",
    "print(\"intervalo (min):\", INTERVAL_MIN)\n",
    "print(\"duración (h):\", DURATION_HOURS)\n",
    "print(\"llamadas esperadas en esta corrida:\", expected_calls)\n",
    "print(\"llamadas esperadas por día con este setup:\", expected_per_day, \"(budget:\", TOMTOM_DAILY_BUDGET, \")\")\n",
    "\n",
    "if expected_per_day > TOMTOM_DAILY_BUDGET:\n",
    "    print(\"ojo: con este setup te pasas del budget diario, baja MAX_POINTS o sube INTERVAL_MIN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56536df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_tomtom_flow(lat: float, lon: float, api_key: str):\n",
    "    url = \"https://api.tomtom.com/traffic/services/4/flowSegmentData/absolute/10/json\"\n",
    "    params = {\n",
    "        \"key\": api_key,\n",
    "        \"point\": f\"{lat},{lon}\",\n",
    "        \"unit\": \"KMPH\",\n",
    "        \"openLr\": \"false\",\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7694c56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guardando en: /Users/davidbazalduamendez/Documents/GitHub/Short-term-traffic-congestion-prediction-for-London-Final-Project/notebooks/data/raw/tomtom_snapshots_20260109T114337Z.jsonl\n",
      "iteraciones: 48\n"
     ]
    }
   ],
   "source": [
    "RAW_DIR = Path(\"data/raw\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_ID = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "OUT_JSONL = RAW_DIR / f\"tomtom_snapshots_{RUN_ID}.jsonl\"\n",
    "\n",
    "INTERVAL_SECONDS = INTERVAL_MIN * 60\n",
    "MAX_ITERS = int((DURATION_HOURS * 3600) / INTERVAL_SECONDS)\n",
    "\n",
    "print(\"guardando en:\", OUT_JSONL.resolve())\n",
    "print(\"iteraciones:\", MAX_ITERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1909edb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iter 1/48  utc=2026-01-09T11:43:38.061733+00:00\n",
      "calls: 50 | errores: 0 | sleep: 589s\n",
      "\n",
      "iter 2/48  utc=2026-01-09T11:53:38.072764+00:00\n",
      "calls: 100 | errores: 0 | sleep: 589s\n",
      "\n",
      "iter 3/48  utc=2026-01-09T12:03:38.088145+00:00\n",
      "calls: 150 | errores: 0 | sleep: 589s\n",
      "\n",
      "iter 4/48  utc=2026-01-09T12:13:38.070988+00:00\n",
      "calls: 200 | errores: 0 | sleep: 589s\n",
      "\n",
      "iter 5/48  utc=2026-01-09T12:23:38.075538+00:00\n",
      "calls: 250 | errores: 0 | sleep: 589s\n",
      "\n",
      "iter 6/48  utc=2026-01-09T12:33:38.083094+00:00\n",
      "calls: 300 | errores: 0 | sleep: 589s\n",
      "\n",
      "iter 7/48  utc=2026-01-09T12:43:38.087838+00:00\n",
      "calls: 350 | errores: 0 | sleep: 590s\n",
      "\n",
      "iter 8/48  utc=2026-01-09T12:53:38.156444+00:00\n",
      "calls: 400 | errores: 0 | sleep: 590s\n",
      "\n",
      "iter 9/48  utc=2026-01-09T13:03:38.171140+00:00\n",
      "calls: 450 | errores: 0 | sleep: 590s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m         sleep_for \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, INTERVAL_SECONDS \u001b[38;5;241m-\u001b[39m elapsed)\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalls: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_calls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | errores: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrors\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | sleep: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(sleep_for)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_for\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def now_utc_iso():\n",
    "    return datetime.now(timezone.utc).isoformat()\n",
    "\n",
    "errors = 0\n",
    "total_calls = 0\n",
    "\n",
    "with OUT_JSONL.open(\"a\", encoding=\"utf-8\") as f:\n",
    "    for it in range(1, MAX_ITERS + 1):\n",
    "        iter_start = time.time()\n",
    "        ts = now_utc_iso()\n",
    "\n",
    "        print(f\"\\niter {it}/{MAX_ITERS}  utc={ts}\")\n",
    "\n",
    "        for row in points.itertuples(index=False):\n",
    "            point_id = str(row.point_id)\n",
    "            lat = float(row.latitude)\n",
    "            lon = float(row.longitude)\n",
    "\n",
    "            record = {\n",
    "                \"timestamp_utc\": ts,\n",
    "                \"point_id\": point_id,\n",
    "                \"latitude\": lat,\n",
    "                \"longitude\": lon,\n",
    "                \"source\": \"tomtom_flowSegmentData\",\n",
    "                \"payload\": None,\n",
    "                \"error\": None\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                record[\"payload\"] = fetch_tomtom_flow(lat, lon, TOMTOM_API_KEY)\n",
    "            except requests.HTTPError as e:\n",
    "                status = getattr(e.response, \"status_code\", None)\n",
    "                record[\"error\"] = f\"HTTPError {status}: {str(e)[:200]}\"\n",
    "                errors += 1\n",
    "                if status == 429:\n",
    "                    time.sleep(10)  # backoff si te rate-limitea\n",
    "            except Exception as e:\n",
    "                record[\"error\"] = f\"{type(e).__name__}: {str(e)[:200]}\"\n",
    "                errors += 1\n",
    "\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "            total_calls += 1\n",
    "\n",
    "            time.sleep(0.05)  # micro-sleep\n",
    "\n",
    "        elapsed = time.time() - iter_start\n",
    "        sleep_for = max(0, INTERVAL_SECONDS - elapsed)\n",
    "        print(f\"calls: {total_calls} | errores: {errors} | sleep: {int(sleep_for)}s\")\n",
    "        time.sleep(sleep_for)\n",
    "\n",
    "print(\"listo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a48751",
   "metadata": {},
   "source": [
    "## Build processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a6961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/raw/tomtom_snapshots_20260104T210243Z.jsonl', 'data/raw/tomtom_snapshots_20260104T210624Z.jsonl', 'data/raw/tomtom_snapshots_20260105T101732Z.jsonl', 'data/raw/tomtom_snapshots_20260105T133619Z.jsonl', 'data/raw/tomtom_snapshots_20260105T232705Z.jsonl', 'data/raw/tomtom_snapshots_20260106T083257Z.jsonl', 'data/raw/tomtom_snapshots_20260106T175950Z.jsonl', 'data/raw/tomtom_snapshots_20260106T234001Z.jsonl', 'data/raw/tomtom_snapshots_20260107T085543Z.jsonl', 'data/raw/tomtom_snapshots_20260107T203033Z.jsonl', 'data/raw/tomtom_snapshots_20260107T215059Z.jsonl', 'data/raw/tomtom_snapshots_20260108T095052Z.jsonl', 'data/raw/tomtom_snapshots_20260108T224525Z.jsonl']\n",
      "shape: (22950, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp_utc</th>\n",
       "      <th>point_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>source</th>\n",
       "      <th>payload</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-04T21:03:15.487051+00:00</td>\n",
       "      <td>18526</td>\n",
       "      <td>51.529441</td>\n",
       "      <td>0.088977</td>\n",
       "      <td>tomtom_flowSegmentData</td>\n",
       "      <td>{'flowSegmentData': {'frc': 'FRC2', 'currentSp...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-04T21:03:15.487051+00:00</td>\n",
       "      <td>7518</td>\n",
       "      <td>51.544222</td>\n",
       "      <td>0.147921</td>\n",
       "      <td>tomtom_flowSegmentData</td>\n",
       "      <td>{'flowSegmentData': {'frc': 'FRC3', 'currentSp...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-04T21:03:15.487051+00:00</td>\n",
       "      <td>6797</td>\n",
       "      <td>51.367362</td>\n",
       "      <td>0.045676</td>\n",
       "      <td>tomtom_flowSegmentData</td>\n",
       "      <td>{'flowSegmentData': {'frc': 'FRC2', 'currentSp...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-04T21:03:15.487051+00:00</td>\n",
       "      <td>36807</td>\n",
       "      <td>51.482110</td>\n",
       "      <td>0.101649</td>\n",
       "      <td>tomtom_flowSegmentData</td>\n",
       "      <td>{'flowSegmentData': {'frc': 'FRC3', 'currentSp...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2026-01-04T21:03:15.487051+00:00</td>\n",
       "      <td>28510</td>\n",
       "      <td>51.501415</td>\n",
       "      <td>-0.075800</td>\n",
       "      <td>tomtom_flowSegmentData</td>\n",
       "      <td>{'flowSegmentData': {'frc': 'FRC3', 'currentSp...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      timestamp_utc point_id   latitude  longitude  \\\n",
       "0  2026-01-04T21:03:15.487051+00:00    18526  51.529441   0.088977   \n",
       "1  2026-01-04T21:03:15.487051+00:00     7518  51.544222   0.147921   \n",
       "2  2026-01-04T21:03:15.487051+00:00     6797  51.367362   0.045676   \n",
       "3  2026-01-04T21:03:15.487051+00:00    36807  51.482110   0.101649   \n",
       "4  2026-01-04T21:03:15.487051+00:00    28510  51.501415  -0.075800   \n",
       "\n",
       "                   source                                            payload  \\\n",
       "0  tomtom_flowSegmentData  {'flowSegmentData': {'frc': 'FRC2', 'currentSp...   \n",
       "1  tomtom_flowSegmentData  {'flowSegmentData': {'frc': 'FRC3', 'currentSp...   \n",
       "2  tomtom_flowSegmentData  {'flowSegmentData': {'frc': 'FRC2', 'currentSp...   \n",
       "3  tomtom_flowSegmentData  {'flowSegmentData': {'frc': 'FRC3', 'currentSp...   \n",
       "4  tomtom_flowSegmentData  {'flowSegmentData': {'frc': 'FRC3', 'currentSp...   \n",
       "\n",
       "  error  \n",
       "0  None  \n",
       "1  None  \n",
       "2  None  \n",
       "3  None  \n",
       "4  None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from glob import glob\n",
    "\n",
    "paths = sorted(glob(\"data/raw/tomtom_snapshots_*.jsonl\"))\n",
    "print(paths)\n",
    "\n",
    "rows = []\n",
    "for p in paths:\n",
    "    with open(p, \"r\") as f:\n",
    "        for line in f:\n",
    "            rows.append(json.loads(line))\n",
    "\n",
    "df_raw = pd.DataFrame(rows)\n",
    "print(\"shape:\", df_raw.shape)\n",
    "df_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f6cd6",
   "metadata": {},
   "source": [
    "## Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b507753f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "point_id\n",
      "28092    397\n",
      "26664    397\n",
      "9029     396\n",
      "6797     396\n",
      "37718    396\n",
      "36799    396\n",
      "36666    396\n",
      "36664    396\n",
      "6760     396\n",
      "28383    396\n",
      "Name: timestamp_utc, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "obs_per_point = (\n",
    "    df_raw[df_raw[\"error\"].isna()]\n",
    "    .groupby(\"point_id\")[\"timestamp_utc\"]\n",
    "    .nunique()\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(obs_per_point.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
