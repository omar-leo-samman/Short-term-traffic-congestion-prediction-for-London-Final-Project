{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed3b9a4",
   "metadata": {},
   "source": [
    "# 07 — Realtime Feature Update Job (Modal, every 30 minutes)\n",
    "\n",
    "This notebook is designed to run **headlessly** (e.g., on Modal) every 30 minutes.\n",
    "\n",
    "It:\n",
    "1. Logs into Hopsworks\n",
    "2. Loads monitoring points from metadata Feature Group\n",
    "3. Pulls **current** TomTom traffic data for each point\n",
    "4. Inserts raw rows into `traffic_flow_fg` (10-minute bucket `ts_10m`)\n",
    "5. Computes the **same engineered features** as your historical backfill notebook and inserts them into `traffic_temporal_fg`\n",
    "6. Updates hourly **weather** features from Open-Meteo and hourly **TfL disruptions** features (optional)\n",
    "\n",
    "### Notes on consistency\n",
    "Your historical engineering uses rolling windows defined in **number of rows** (10-min buckets).\n",
    "If you only fetch TomTom every 30 minutes, you will have fewer observations and the *effective time span*\n",
    "of a “window=3” rolling feature will become ~90 minutes.\n",
    "\n",
    "If you need strict *time-based* window semantics (e.g., exactly 30/60/120 minutes), you should either:\n",
    "- collect every 10 minutes, OR\n",
    "- redefine rolling features in minutes and backfill them consistently.\n",
    "\n",
    "For now, this notebook reproduces the **same feature definitions** as in the historical code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd95ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# If you run this notebook outside an environment that already has dependencies,\n",
    "# uncomment the following line:\n",
    "# !pip install -U hopsworks pandas numpy requests\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import hopsworks\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c90069",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f893c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_ts: 2026-01-11 13:17:03.077481+00:00\n",
      "ts_10m: 2026-01-11 13:10:00+00:00\n",
      "weather_time_utc: 2026-01-11 13:00:00+00:00\n",
      "tfl_time_utc: 2026-01-11 13:00:00+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Run time\n",
    "# -------------------------\n",
    "# For Modal/papermill runs you can pass RUN_TS_UTC=\"2026-01-09T12:30:00Z\"\n",
    "RUN_TS_UTC = os.getenv(\"RUN_TS_UTC\", \"\")\n",
    "if RUN_TS_UTC:\n",
    "    run_ts = pd.to_datetime(RUN_TS_UTC, utc=True, errors=\"raise\")\n",
    "else:\n",
    "    run_ts = pd.Timestamp.now(tz=\"UTC\")\n",
    "\n",
    "# Use the same 10-minute bucketing as your raw Feature Group\n",
    "ts_10m = run_ts.floor(\"10min\")\n",
    "\n",
    "# Hourly buckets for weather and TfL\n",
    "weather_time_utc = ts_10m.floor(\"H\")\n",
    "tfl_time_utc = ts_10m.floor(\"H\")\n",
    "\n",
    "print(\"run_ts:\", run_ts)\n",
    "print(\"ts_10m:\", ts_10m)\n",
    "print(\"weather_time_utc:\", weather_time_utc)\n",
    "print(\"tfl_time_utc:\", tfl_time_utc)\n",
    "\n",
    "# -------------------------\n",
    "# Hopsworks connection\n",
    "# -------------------------\n",
    "HOPSWORKS_HOST = os.getenv(\"HOPSWORKS_HOST\", \"\")\n",
    "HOPSWORKS_PROJECT = os.getenv(\"HOPSWORKS_PROJECT\", \"\")\n",
    "HOPSWORKS_API_KEY = os.getenv(\"HOPSWORKS_API_KEY\", \"\")\n",
    "\n",
    "# -------------------------\n",
    "# API Keys\n",
    "# -------------------------\n",
    "TOMTOM_API_KEY = os.getenv(\"TOMTOM_API_KEY\", \"\")\n",
    "TFL_APP_ID = os.getenv(\"TFL_APP_ID\", \"\")\n",
    "TFL_APP_KEY = os.getenv(\"TFL_APP_KEY\", \"\")\n",
    "\n",
    "# -------------------------\n",
    "# Feature Groups (defaults aligned with your existing notebooks)\n",
    "# -------------------------\n",
    "RAW_FG_NAME = os.getenv(\"RAW_FG_NAME\", \"traffic_flow_fg\")\n",
    "RAW_FG_VERSION = int(os.getenv(\"RAW_FG_VERSION\", \"1\"))\n",
    "\n",
    "ENGINEERED_FG_NAME = os.getenv(\"ENGINEERED_FG_NAME\", \"traffic_temporal_fg\")\n",
    "ENGINEERED_FG_VERSION = int(os.getenv(\"ENGINEERED_FG_VERSION\", \"1\"))\n",
    "\n",
    "METADATA_FG_NAME = os.getenv(\"METADATA_FG_NAME\", \"traffic_points_metadata\")\n",
    "METADATA_FG_VERSION = int(os.getenv(\"METADATA_FG_VERSION\", \"1\"))\n",
    "\n",
    "WEATHER_FG_NAME = os.getenv(\"WEATHER_FG_NAME\", \"weather_10m_fg\")\n",
    "WEATHER_FG_VERSION = int(os.getenv(\"WEATHER_FG_VERSION\", \"1\"))\n",
    "\n",
    "TFL_FG_NAME = os.getenv(\"TFL_FG_NAME\", \"tfl_disruptions_10m_fg\")\n",
    "TFL_FG_VERSION = int(os.getenv(\"TFL_FG_VERSION\", \"1\"))\n",
    "\n",
    "# -------------------------\n",
    "# Feature engineering params (match your backfill defaults)\n",
    "# -------------------------\n",
    "LOW_CONF_THRESHOLD = float(os.getenv(\"LOW_CONF_THRESHOLD\", \"0.7\"))\n",
    "ROLL_WINDOWS = [3, 6, 12]  # designed for 10-min buckets\n",
    "\n",
    "# -------------------------\n",
    "# Controls\n",
    "# -------------------------\n",
    "INSERT_RAW = os.getenv(\"INSERT_RAW\", \"1\") == \"1\"\n",
    "INSERT_ENGINEERED = os.getenv(\"INSERT_ENGINEERED\", \"1\") == \"1\"\n",
    "UPDATE_WEATHER = os.getenv(\"UPDATE_WEATHER\", \"1\") == \"1\"\n",
    "UPDATE_TFL = os.getenv(\"UPDATE_TFL\", \"1\") == \"1\"\n",
    "\n",
    "LOOKBACK_HOURS = int(os.getenv(\"LOOKBACK_HOURS\", \"24\"))  # used to compute rolling features reliably\n",
    "\n",
    "assert TOMTOM_API_KEY, \"Missing TOMTOM_API_KEY env var\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab3bfb3",
   "metadata": {},
   "source": [
    "## 2) Login to Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33ead12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 14:17:05,863 INFO: Initializing external client\n",
      "2026-01-11 14:17:05,863 INFO: Base URL: https://eu-west.cloud.hopsworks.ai:443\n",
      "2026-01-11 14:17:07,138 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://eu-west.cloud.hopsworks.ai:443/p/3209\n",
      "Connected. Feature store: london_traffic_featurestore\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login(\n",
    "    host=\"eu-west.cloud.hopsworks.ai\",\n",
    "    project=\"London_traffic\"\n",
    ")\n",
    "fs = project.get_feature_store()\n",
    "raw_fg = fs.get_feature_group(name=RAW_FG_NAME, version=RAW_FG_VERSION)\n",
    "engineered_fg = fs.get_feature_group(name=ENGINEERED_FG_NAME, version=ENGINEERED_FG_VERSION)\n",
    "meta_fg = fs.get_feature_group(name=METADATA_FG_NAME, version=METADATA_FG_VERSION)\n",
    "\n",
    "print(\"Connected. Feature store:\", fs.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28042fd5",
   "metadata": {},
   "source": [
    "## 3) Load monitoring points (metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be267348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.26s) \n",
      "Loaded points: 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8618</td>\n",
       "      <td>51.509307</td>\n",
       "      <td>-0.084878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16756</td>\n",
       "      <td>51.383489</td>\n",
       "      <td>-0.105944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38815</td>\n",
       "      <td>51.393451</td>\n",
       "      <td>0.029589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6458</td>\n",
       "      <td>51.573103</td>\n",
       "      <td>-0.212077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38022</td>\n",
       "      <td>51.589213</td>\n",
       "      <td>0.270734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  point_id   latitude  longitude\n",
       "0     8618  51.509307  -0.084878\n",
       "1    16756  51.383489  -0.105944\n",
       "2    38815  51.393451   0.029589\n",
       "3     6458  51.573103  -0.212077\n",
       "4    38022  51.589213   0.270734"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df = meta_fg.read()\n",
    "required = {\"point_id\", \"latitude\", \"longitude\"}\n",
    "missing = required - set(meta_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Metadata FG missing required columns: {missing}\")\n",
    "\n",
    "points = meta_df[[\"point_id\", \"latitude\", \"longitude\"]].copy()\n",
    "points[\"point_id\"] = points[\"point_id\"].astype(str)\n",
    "points[\"latitude\"] = pd.to_numeric(points[\"latitude\"], errors=\"coerce\")\n",
    "points[\"longitude\"] = pd.to_numeric(points[\"longitude\"], errors=\"coerce\")\n",
    "points = points.dropna(subset=[\"point_id\", \"latitude\", \"longitude\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded points:\", len(points))\n",
    "points.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf7fd0",
   "metadata": {},
   "source": [
    "## 4) Fetch TomTom Flow data for all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec4c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw tick df shape: (50, 12) errors: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_id</th>\n",
       "      <th>timestamp_utc</th>\n",
       "      <th>ts_10m</th>\n",
       "      <th>current_speed</th>\n",
       "      <th>free_flow_speed</th>\n",
       "      <th>frc</th>\n",
       "      <th>current_travel_time</th>\n",
       "      <th>free_flow_travel_time</th>\n",
       "      <th>confidence</th>\n",
       "      <th>road_closure</th>\n",
       "      <th>speed_ratio</th>\n",
       "      <th>delay_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8618</td>\n",
       "      <td>2026-01-11 13:17:03.077481+00:00</td>\n",
       "      <td>2026-01-11 13:10:00+00:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>FRC2</td>\n",
       "      <td>572.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16756</td>\n",
       "      <td>2026-01-11 13:17:03.077481+00:00</td>\n",
       "      <td>2026-01-11 13:10:00+00:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>FRC3</td>\n",
       "      <td>149.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>102.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38815</td>\n",
       "      <td>2026-01-11 13:17:03.077481+00:00</td>\n",
       "      <td>2026-01-11 13:10:00+00:00</td>\n",
       "      <td>33.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>FRC3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6458</td>\n",
       "      <td>2026-01-11 13:17:03.077481+00:00</td>\n",
       "      <td>2026-01-11 13:10:00+00:00</td>\n",
       "      <td>50.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>FRC2</td>\n",
       "      <td>169.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38022</td>\n",
       "      <td>2026-01-11 13:17:03.077481+00:00</td>\n",
       "      <td>2026-01-11 13:10:00+00:00</td>\n",
       "      <td>108.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>FRC0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  point_id                    timestamp_utc                    ts_10m  current_speed  free_flow_speed   frc  current_travel_time  free_flow_travel_time  confidence  road_closure  speed_ratio  \\\n",
       "0     8618 2026-01-11 13:17:03.077481+00:00 2026-01-11 13:10:00+00:00           23.0             29.0  FRC2                572.0                  454.0         1.0         False     0.793103   \n",
       "1    16756 2026-01-11 13:17:03.077481+00:00 2026-01-11 13:10:00+00:00            6.0             19.0  FRC3                149.0                   47.0         1.0         False     0.315789   \n",
       "2    38815 2026-01-11 13:17:03.077481+00:00 2026-01-11 13:10:00+00:00           33.0             43.0  FRC3                 49.0                   38.0         1.0         False     0.767442   \n",
       "3     6458 2026-01-11 13:17:03.077481+00:00 2026-01-11 13:10:00+00:00           50.0             58.0  FRC2                169.0                  146.0         1.0         False     0.862069   \n",
       "4    38022 2026-01-11 13:17:03.077481+00:00 2026-01-11 13:10:00+00:00          108.0            108.0  FRC0                132.0                  132.0         1.0         False     1.000000   \n",
       "\n",
       "   delay_seconds  \n",
       "0          118.0  \n",
       "1          102.0  \n",
       "2           11.0  \n",
       "3           23.0  \n",
       "4            0.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_POINTS = 50\n",
    "\n",
    "points_subset = (\n",
    "    points\n",
    "    .dropna(subset=[\"latitude\", \"longitude\"])\n",
    "    .head(MAX_POINTS)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "def fetch_tomtom_flow(lat: float, lon: float, *, zoom: int = 10, unit: str = \"kmph\") -> Dict:\n",
    "    # TomTom Flow Segment Data API (v4)\n",
    "    url = f\"https://api.tomtom.com/traffic/services/4/flowSegmentData/absolute/{zoom}/json\"\n",
    "    params = {\n",
    "        \"key\": TOMTOM_API_KEY,\n",
    "        \"point\": f\"{lat},{lon}\",\n",
    "        \"unit\": unit,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def parse_tomtom_payload(payload: Dict) -> Dict:\n",
    "    fsd = payload.get(\"flowSegmentData\", {}) if isinstance(payload, dict) else {}\n",
    "    frc = fsd.get(\"frc\")\n",
    "    current_speed = fsd.get(\"currentSpeed\")\n",
    "    free_flow_speed = fsd.get(\"freeFlowSpeed\")\n",
    "    current_tt = fsd.get(\"currentTravelTime\")\n",
    "    free_flow_tt = fsd.get(\"freeFlowTravelTime\")\n",
    "    confidence = fsd.get(\"confidence\")\n",
    "    road_closure = fsd.get(\"roadClosure\")\n",
    "\n",
    "    # Derived\n",
    "    speed_ratio = None\n",
    "    delay_seconds = None\n",
    "    try:\n",
    "        if current_speed is not None and free_flow_speed:\n",
    "            speed_ratio = float(current_speed) / float(free_flow_speed)\n",
    "    except Exception:\n",
    "        speed_ratio = None\n",
    "\n",
    "    try:\n",
    "        if current_tt is not None and free_flow_tt is not None:\n",
    "            delay_seconds = float(current_tt) - float(free_flow_tt)\n",
    "    except Exception:\n",
    "        delay_seconds = None\n",
    "\n",
    "    return {\n",
    "        \"current_speed\": float(current_speed) if current_speed is not None else np.nan,\n",
    "        \"free_flow_speed\": float(free_flow_speed) if free_flow_speed is not None else np.nan,\n",
    "        \"frc\": str(frc) if frc is not None else np.nan,\n",
    "        \"current_travel_time\": float(current_tt) if current_tt is not None else np.nan,\n",
    "        \"free_flow_travel_time\": float(free_flow_tt) if free_flow_tt is not None else np.nan,\n",
    "        \"confidence\": float(confidence) if confidence is not None else np.nan,\n",
    "        \"road_closure\": bool(road_closure) if road_closure is not None else False,\n",
    "        \"speed_ratio\": float(speed_ratio) if speed_ratio is not None else np.nan,\n",
    "        \"delay_seconds\": float(delay_seconds) if delay_seconds is not None else np.nan,\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "errors = 0\n",
    "\n",
    "for _, p in points_subset.iterrows():\n",
    "    pid = str(p[\"point_id\"])\n",
    "    lat = float(p[\"latitude\"])\n",
    "    lon = float(p[\"longitude\"])\n",
    "    try:\n",
    "        payload = fetch_tomtom_flow(lat, lon)\n",
    "        parsed = parse_tomtom_payload(payload)\n",
    "        rows.append({\n",
    "            \"point_id\": pid,\n",
    "            \"timestamp_utc\": run_ts,\n",
    "            \"ts_10m\": ts_10m,\n",
    "            **parsed,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        rows.append({\n",
    "            \"point_id\": pid,\n",
    "            \"timestamp_utc\": run_ts,\n",
    "            \"ts_10m\": ts_10m,\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "\n",
    "raw_tick_df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Raw tick df shape:\", raw_tick_df.shape, \"errors:\", errors)\n",
    "\n",
    "raw_tick_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f136b",
   "metadata": {},
   "source": [
    "## 5) Insert RAW tick into `traffic_flow_fg` (optional, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f063d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.62s) \n",
      "Raw rows to insert: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 50/50 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: traffic_flow_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://eu-west.cloud.hopsworks.ai:443/p/3209/jobs/named/traffic_flow_fg_1_offline_fg_materialization/executions\n",
      "Inserted raw tick.\n"
     ]
    }
   ],
   "source": [
    "if INSERT_RAW:\n",
    "    # Keep only columns expected by your raw FG schema. If schema differs, adjust here.\n",
    "    # We also ensure road_closure is numpy bool (not pandas nullable boolean).\n",
    "    keep_cols = [\n",
    "        \"point_id\", \"timestamp_utc\", \"ts_10m\", \"frc\",\n",
    "        \"current_speed\", \"free_flow_speed\", \"current_travel_time\", \"free_flow_travel_time\",\n",
    "        \"confidence\", \"road_closure\", \"speed_ratio\", \"delay_seconds\"\n",
    "    ]\n",
    "    missing = [c for c in keep_cols if c not in raw_tick_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"raw_tick_df missing columns: {missing}\")\n",
    "\n",
    "    df_ins = raw_tick_df[keep_cols].copy()\n",
    "    df_ins[\"point_id\"] = df_ins[\"point_id\"].astype(str)\n",
    "    df_ins[\"timestamp_utc\"] = pd.to_datetime(df_ins[\"timestamp_utc\"], utc=True)\n",
    "    df_ins[\"ts_10m\"] = pd.to_datetime(df_ins[\"ts_10m\"], utc=True)\n",
    "\n",
    "    # Fix Avro bool edge case\n",
    "    df_ins[\"road_closure\"] = df_ins[\"road_closure\"].fillna(False).astype(bool)\n",
    "\n",
    "    # Deduplicate on PK\n",
    "    df_ins = df_ins.drop_duplicates(subset=[\"point_id\", \"ts_10m\"], keep=\"last\")\n",
    "\n",
    "    # Avoid inserting duplicates if rerun\n",
    "    existing_raw = raw_fg.read()\n",
    "    existing_raw[\"ts_10m\"] = pd.to_datetime(existing_raw[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "    existing_keys = set(zip(existing_raw[\"point_id\"].astype(str), existing_raw[\"ts_10m\"]))\n",
    "    df_ins[\"__key\"] = list(zip(df_ins[\"point_id\"].astype(str), df_ins[\"ts_10m\"]))\n",
    "    df_ins = df_ins[~df_ins[\"__key\"].isin(existing_keys)].drop(columns=\"__key\")\n",
    "\n",
    "    print(\"Raw rows to insert:\", len(df_ins))\n",
    "    if len(df_ins) > 0:\n",
    "        raw_fg.insert(df_ins)\n",
    "        print(\"Inserted raw tick.\")\n",
    "    else:\n",
    "        print(\"No new raw rows (already present).\")\n",
    "else:\n",
    "    print(\"Skipping RAW insert (INSERT_RAW=0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa6522",
   "metadata": {},
   "source": [
    "## 6) Compute engineered features for this tick and insert into `traffic_temporal_fg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1198bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.71s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.82s) \n",
      "Engineered rows to insert: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 50/50 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: traffic_temporal_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://eu-west.cloud.hopsworks.ai:443/p/3209/jobs/named/traffic_temporal_fg_1_offline_fg_materialization/executions\n",
      "Inserted engineered tick.\n"
     ]
    }
   ],
   "source": [
    "def time_interval(hour: int) -> str:\n",
    "    if 7 <= hour < 10:\n",
    "        return \"morning_peak\"\n",
    "    elif 10 <= hour < 16:\n",
    "        return \"midday\"\n",
    "    elif 16 <= hour < 19:\n",
    "        return \"evening_peak\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "\n",
    "def build_engineered_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Ensure types\n",
    "    df[\"point_id\"] = df[\"point_id\"].astype(str)\n",
    "    df[\"ts_10m\"] = pd.to_datetime(df[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "    df[\"timestamp_utc\"] = pd.to_datetime(df.get(\"timestamp_utc\", df[\"ts_10m\"]), utc=True, errors=\"coerce\")\n",
    "\n",
    "    # Sort + dedupe\n",
    "    df = df.dropna(subset=[\"point_id\", \"ts_10m\"])\n",
    "    df = df.sort_values([\"point_id\", \"ts_10m\"]).drop_duplicates(subset=[\"point_id\", \"ts_10m\"], keep=\"last\")\n",
    "\n",
    "    # Temporal features (same as your backfill)\n",
    "    df[\"day_of_week\"] = df[\"ts_10m\"].dt.weekday\n",
    "    df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    df[\"hour\"] = df[\"ts_10m\"].dt.hour\n",
    "    df[\"minute\"] = df[\"ts_10m\"].dt.minute\n",
    "\n",
    "    df[\"is_rush_hour\"] = ((df[\"hour\"].between(7, 9)) | (df[\"hour\"].between(16, 18))).astype(int)\n",
    "\n",
    "    df[\"time_interval\"] = df[\"hour\"].apply(time_interval)\n",
    "    dummies = pd.get_dummies(df[\"time_interval\"], prefix=\"ti\")\n",
    "\n",
    "    # Ensure stable dummy columns\n",
    "    for col in [\"ti_morning_peak\", \"ti_midday\", \"ti_evening_peak\", \"ti_night\"]:\n",
    "        if col not in dummies.columns:\n",
    "            dummies[col] = 0\n",
    "\n",
    "    df = pd.concat([df.drop(columns=[\"time_interval\"]), dummies[[\"ti_morning_peak\",\"ti_midday\",\"ti_evening_peak\",\"ti_night\"]]], axis=1)\n",
    "    #change to bool\n",
    "    df[\"ti_morning_peak\"] = df[\"ti_morning_peak\"].astype(bool)\n",
    "    df[\"ti_midday\"] = df[\"ti_midday\"].astype(bool)\n",
    "    df[\"ti_evening_peak\"] = df[\"ti_evening_peak\"].astype(bool)\n",
    "    df[\"ti_night\"] = df[\"ti_night\"].astype(bool)\n",
    "    # Traffic features (same as your backfill)\n",
    "    eps = 1e-6\n",
    "    df[\"speed_diff\"] = df[\"free_flow_speed\"] - df[\"current_speed\"]\n",
    "    df[\"travel_time_ratio\"] = df[\"current_travel_time\"] / (df[\"free_flow_travel_time\"] + eps)\n",
    "    df[\"low_confidence_flag\"] = (df[\"confidence\"] < LOW_CONF_THRESHOLD).astype(int)\n",
    "    df[\"travel_time_ratio\"] = df[\"travel_time_ratio\"].clip(lower=0, upper=10)\n",
    "\n",
    "    # Rolling features over rows (same as your backfill)\n",
    "    for w in ROLL_WINDOWS:\n",
    "        df[f\"speed_roll_mean_{w}\"] = (\n",
    "            df.groupby(\"point_id\")[\"current_speed\"]\n",
    "              .rolling(window=w, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        df[f\"speed_roll_std_{w}\"] = (\n",
    "            df.groupby(\"point_id\")[\"current_speed\"]\n",
    "              .rolling(window=w, min_periods=1)\n",
    "              .std()\n",
    "              .reset_index(level=0, drop=True)\n",
    "              .fillna(0.0)\n",
    "        )\n",
    "        df[f\"delay_roll_mean_{w}\"] = (\n",
    "            df.groupby(\"point_id\")[\"delay_seconds\"]\n",
    "              .rolling(window=w, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "if INSERT_ENGINEERED:\n",
    "    # Read history for lookback\n",
    "    hist = raw_fg.read()\n",
    "    hist[\"ts_10m\"] = pd.to_datetime(hist[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "    hist[\"timestamp_utc\"] = pd.to_datetime(hist.get(\"timestamp_utc\", hist[\"ts_10m\"]), utc=True, errors=\"coerce\")\n",
    "    hist[\"point_id\"] = hist[\"point_id\"].astype(str)\n",
    "\n",
    "    lookback_start = ts_10m - pd.Timedelta(hours=LOOKBACK_HOURS)\n",
    "    hist = hist[hist[\"ts_10m\"] >= lookback_start].copy()\n",
    "\n",
    "    # Append current tick \n",
    "    current_raw = raw_tick_df.copy()\n",
    "    # Keep only required raw columns for feature engineering\n",
    "    needed = [\n",
    "        \"point_id\", \"timestamp_utc\", \"ts_10m\", \"frc\",\n",
    "        \"current_speed\", \"free_flow_speed\", \"current_travel_time\", \"free_flow_travel_time\",\n",
    "        \"confidence\", \"road_closure\", \"speed_ratio\", \"delay_seconds\"\n",
    "    ]\n",
    "    current_raw = current_raw[needed].copy()\n",
    "    current_raw[\"timestamp_utc\"] = pd.to_datetime(current_raw[\"timestamp_utc\"], utc=True)\n",
    "    current_raw[\"ts_10m\"] = pd.to_datetime(current_raw[\"ts_10m\"], utc=True)\n",
    "\n",
    "    combined = pd.concat([hist, current_raw], ignore_index=True)\n",
    "    combined = combined.drop_duplicates(subset=[\"point_id\", \"ts_10m\"], keep=\"last\")\n",
    "    combined = combined.sort_values([\"point_id\", \"ts_10m\"]).reset_index(drop=True)\n",
    "\n",
    "    eng_all = build_engineered_features(combined)\n",
    "\n",
    "    # Keep only current tick rows\n",
    "    eng_tick = eng_all[eng_all[\"ts_10m\"] == ts_10m].copy()\n",
    "    eng_tick = eng_tick.drop_duplicates(subset=[\"point_id\", \"ts_10m\"], keep=\"last\")\n",
    "\n",
    "    # Align columns to FG schema (important!)\n",
    "    schema_cols = [f.name for f in engineered_fg.features]\n",
    "    for c in schema_cols:\n",
    "        if c not in eng_tick.columns:\n",
    "            eng_tick[c] = np.nan\n",
    "\n",
    "    eng_tick = eng_tick[schema_cols].copy()\n",
    "\n",
    "    # Avoid duplicates if rerun\n",
    "    existing_eng = engineered_fg.read()\n",
    "    existing_eng[\"ts_10m\"] = pd.to_datetime(existing_eng[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "    existing_keys = set(zip(existing_eng[\"point_id\"].astype(str), existing_eng[\"ts_10m\"]))\n",
    "    eng_tick[\"__key\"] = list(zip(eng_tick[\"point_id\"].astype(str), eng_tick[\"ts_10m\"]))\n",
    "    eng_tick = eng_tick[~eng_tick[\"__key\"].isin(existing_keys)].drop(columns=\"__key\")\n",
    "\n",
    "    print(\"Engineered rows to insert:\", len(eng_tick))\n",
    "    if len(eng_tick) > 0:\n",
    "        engineered_fg.insert(eng_tick)\n",
    "        print(\"Inserted engineered tick.\")\n",
    "    else:\n",
    "        print(\"No new engineered rows (already present).\")\n",
    "else:\n",
    "    print(\"Skipping engineered insert (INSERT_ENGINEERED=0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01891f9",
   "metadata": {},
   "source": [
    "## 7) Update hourly Weather features from Open-Meteo (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a4de6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (2.74s) \n",
      "No new weather_10m rows for this hour window (already present).\n"
     ]
    }
   ],
   "source": [
    "def fetch_openmeteo_hourly(lat: float, lon: float, hour_utc: pd.Timestamp) -> Dict[str, float]:\n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    hourly_vars = [\"temperature_2m\", \"precipitation\", \"rain\", \"snowfall\"]\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"hourly\": \",\".join(hourly_vars),\n",
    "        \"timezone\": \"UTC\",\n",
    "        \"past_days\": 1,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "\n",
    "    times = js.get(\"hourly\", {}).get(\"time\", [])\n",
    "    if not times:\n",
    "        raise RuntimeError(\"Open-Meteo returned no hourly times\")\n",
    "\n",
    "    t = pd.to_datetime(times, utc=True, errors=\"coerce\")\n",
    "    idx = int(np.argmin(np.abs((t - hour_utc).total_seconds())))\n",
    "\n",
    "    out = {}\n",
    "    for v in hourly_vars:\n",
    "        arr = js.get(\"hourly\", {}).get(v, [])\n",
    "        out[v] = float(arr[idx]) if idx < len(arr) and arr[idx] is not None else np.nan\n",
    "    return out\n",
    "\n",
    "\n",
    "def expand_hour_to_10m_timestamps(hour_utc: pd.Timestamp) -> list[pd.Timestamp]:\n",
    "    hour_utc = pd.to_datetime(hour_utc, utc=True, errors=\"coerce\").floor(\"H\")\n",
    "    offsets = pd.to_timedelta([0, 10, 20, 30, 40, 50], unit=\"m\")\n",
    "    return [hour_utc + off for off in offsets]\n",
    "\n",
    "\n",
    "if UPDATE_WEATHER:\n",
    "    weather_10m_fg = fs.get_feature_group(name=\"weather_10m_fg\", version=1)\n",
    "\n",
    "    weather_time_utc = pd.to_datetime(weather_time_utc, utc=True, errors=\"coerce\").floor(\"H\")\n",
    "    ts_10m_list = expand_hour_to_10m_timestamps(weather_time_utc)\n",
    "\n",
    "    # Existing keys only for this hour window\n",
    "    existing_weather = weather_10m_fg.read(read_options={\"use_hive\": True})\n",
    "    existing_weather[\"ts_10m\"] = pd.to_datetime(existing_weather[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "    existing_weather[\"point_id\"] = existing_weather[\"point_id\"].astype(str)\n",
    "    existing_weather = existing_weather[existing_weather[\"ts_10m\"].isin(ts_10m_list)]\n",
    "    existing_keys = set(zip(existing_weather[\"point_id\"], existing_weather[\"ts_10m\"]))\n",
    "\n",
    "    weather_rows = []\n",
    "    for _, p in points.iterrows():\n",
    "        pid = str(p[\"point_id\"])\n",
    "        lat = float(p[\"latitude\"])\n",
    "        lon = float(p[\"longitude\"])\n",
    "\n",
    "        try:\n",
    "            vals = fetch_openmeteo_hourly(lat, lon, weather_time_utc)\n",
    "\n",
    "            for ts_10m in ts_10m_list:\n",
    "                key = (pid, ts_10m)\n",
    "                if key in existing_keys:\n",
    "                    continue\n",
    "                weather_rows.append({\n",
    "                    \"point_id\": pid,\n",
    "                    \"ts_10m\": ts_10m,\n",
    "                    \"precipitation\": vals.get(\"precipitation\", np.nan),\n",
    "                    \"rain\": vals.get(\"rain\", np.nan),\n",
    "                    \"snowfall\": vals.get(\"snowfall\", np.nan),\n",
    "                    \"temperature_2m\": vals.get(\"temperature_2m\", np.nan),\n",
    "                })\n",
    "\n",
    "        except Exception:\n",
    "            # No error column in schema: insert NaNs\n",
    "            for ts_10m in ts_10m_list:\n",
    "                key = (pid, ts_10m)\n",
    "                if key in existing_keys:\n",
    "                    continue\n",
    "                weather_rows.append({\n",
    "                    \"point_id\": pid,\n",
    "                    \"ts_10m\": ts_10m,\n",
    "                    \"precipitation\": np.nan,\n",
    "                    \"rain\": np.nan,\n",
    "                    \"snowfall\": np.nan,\n",
    "                    \"temperature_2m\": np.nan,\n",
    "                })\n",
    "\n",
    "    weather_df_new = pd.DataFrame(weather_rows)\n",
    "\n",
    "    if len(weather_df_new) > 0:\n",
    "        # Ensure correct column order\n",
    "        weather_df_new[\"point_id\"] = weather_df_new[\"point_id\"].astype(str)\n",
    "        weather_df_new[\"ts_10m\"] = pd.to_datetime(weather_df_new[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "        weather_df_new = weather_df_new[\n",
    "            [\"point_id\", \"ts_10m\", \"precipitation\", \"rain\", \"snowfall\", \"temperature_2m\"]\n",
    "        ]\n",
    "\n",
    "        print(\"Weather 10-min rows to insert:\", len(weather_df_new))\n",
    "        weather_10m_fg.insert(weather_df_new, write_options={\"wait_for_job\": True})\n",
    "        print(\"Inserted weather_10m for hour:\", weather_time_utc)\n",
    "    else:\n",
    "        print(\"No new weather_10m rows for this hour window (already present).\")\n",
    "else:\n",
    "    print(\"Skipping weather update (UPDATE_WEATHER=0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ddc76",
   "metadata": {},
   "source": [
    "## 8) Update hourly TfL disruption features (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adec8481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfL 10-min rows to upsert: 1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 1200/1200 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: tfl_disruptions_10m_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://eu-west.cloud.hopsworks.ai:443/p/3209/jobs/named/tfl_disruptions_10m_fg_1_offline_fg_materialization/executions\n",
      "2026-01-11 14:23:59,409 INFO: Waiting for execution to finish. Current state: SUBMITTED. Final status: UNDEFINED\n",
      "2026-01-11 14:24:02,567 INFO: Waiting for execution to finish. Current state: RUNNING. Final status: UNDEFINED\n",
      "2026-01-11 14:26:18,732 INFO: Waiting for execution to finish. Current state: SUCCEEDING. Final status: UNDEFINED\n",
      "2026-01-11 14:26:21,907 INFO: Waiting for execution to finish. Current state: FINISHED. Final status: SUCCEEDED\n",
      "2026-01-11 14:26:22,713 INFO: Waiting for log aggregation to finish.\n",
      "2026-01-11 14:26:22,714 INFO: Execution finished successfully.\n",
      "Upserted TfL 10-min rows for hour: 2026-01-11 13:00:00+00:00\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from typing import Optional\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R*c\n",
    "\n",
    "def extract_point_latlon(point_obj) -> Optional[tuple]:\n",
    "    if not isinstance(point_obj, dict):\n",
    "        return None\n",
    "    lat = point_obj.get(\"lat\", None)\n",
    "    lon = point_obj.get(\"lon\", None)\n",
    "    if lat is None or lon is None:\n",
    "        lat = point_obj.get(\"latitude\", None)\n",
    "        lon = point_obj.get(\"longitude\", None)\n",
    "    try:\n",
    "        if lat is None or lon is None:\n",
    "            return None\n",
    "        return (float(lat), float(lon))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fetch_tfl_active_road_disruptions() -> pd.DataFrame:\n",
    "    url = \"https://api.tfl.gov.uk/Road/all/Disruption\"\n",
    "    params = {\n",
    "        \"stripContent\": True,\n",
    "        \"closures\": True,\n",
    "    }\n",
    "    if TFL_APP_ID:\n",
    "        params[\"app_id\"] = TFL_APP_ID\n",
    "    if TFL_APP_KEY:\n",
    "        params[\"app_key\"] = TFL_APP_KEY\n",
    "\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    if r.status_code >= 400:\n",
    "        raise RuntimeError(f\"TfL {r.status_code}: {r.text[:500]}\")\n",
    "    r.raise_for_status()\n",
    "    raw = r.json()\n",
    "    return pd.DataFrame(raw if isinstance(raw, list) else [])\n",
    "\n",
    "RADIUS_KM = float(os.getenv(\"TFL_RADIUS_KM\", \"0.5\"))\n",
    "\n",
    "def expand_hour_to_10m_timestamps(hour_utc: pd.Timestamp) -> list[pd.Timestamp]:\n",
    "    hour_utc = pd.to_datetime(hour_utc, utc=True, errors=\"coerce\").floor(\"H\")\n",
    "    offsets = pd.to_timedelta([0, 10, 20, 30, 40, 50], unit=\"m\")\n",
    "    return [hour_utc + off for off in offsets]\n",
    "\n",
    "\n",
    "if UPDATE_TFL:\n",
    "    if not TFL_APP_KEY and not TFL_APP_ID:\n",
    "        print(\"Skipping TfL update because TFL_APP_KEY/TFL_APP_ID are not set.\")\n",
    "    else:\n",
    "        # IMPORTANT: point to your 10-min FG\n",
    "        tfl_10m_fg = fs.get_feature_group(name=\"tfl_disruptions_10m_fg\", version=1)\n",
    "\n",
    "        # Define hour anchor (hourly source)\n",
    "        hour_start = pd.to_datetime(tfl_time_utc, utc=True, errors=\"coerce\").floor(\"H\")\n",
    "        hour_end = hour_start + pd.Timedelta(hours=1)\n",
    "\n",
    "        ts_10m_list = expand_hour_to_10m_timestamps(hour_start)\n",
    "\n",
    "        # Points\n",
    "        pts = points.copy()\n",
    "        pts[\"point_id\"] = pts[\"point_id\"].astype(str)\n",
    "        pts = pts.dropna(subset=[\"latitude\", \"longitude\"]).reset_index(drop=True)\n",
    "\n",
    "        # Fetch current disruptions\n",
    "        raw_df = fetch_tfl_active_road_disruptions()\n",
    "\n",
    "        # If nothing returned -> zeros for all points (replicated to 10-min)\n",
    "        if raw_df.empty:\n",
    "            base_out = pd.DataFrame({\n",
    "                \"point_id\": pts[\"point_id\"],\n",
    "                \"disruption_count\": 0,\n",
    "                \"is_works\": 0,\n",
    "                \"is_incident\": 0,\n",
    "                \"is_active\": 0,\n",
    "                \"max_ordinal\": 0,\n",
    "            })\n",
    "        else:\n",
    "            # Parse disruption times\n",
    "            raw_df[\"start\"] = pd.to_datetime(raw_df.get(\"startDateTime\"), utc=True, errors=\"coerce\")\n",
    "            raw_df[\"end\"] = pd.to_datetime(raw_df.get(\"endDateTime\"), utc=True, errors=\"coerce\")\n",
    "            raw_df[\"end_filled\"] = raw_df[\"end\"]\n",
    "            raw_df.loc[raw_df[\"end_filled\"].isna(), \"end_filled\"] = raw_df[\"start\"] + pd.Timedelta(hours=1)\n",
    "\n",
    "            # Keep disruptions overlapping this hour window\n",
    "            raw_df = raw_df[\n",
    "                (raw_df[\"start\"].isna()) | ((raw_df[\"start\"] < hour_end) & (raw_df[\"end_filled\"] >= hour_start))\n",
    "            ].copy()\n",
    "\n",
    "            # Extract disruption point coords\n",
    "            coords = raw_df.get(\"point\", pd.Series([None]*len(raw_df))).apply(extract_point_latlon)\n",
    "            raw_df[\"d_lat\"] = coords.apply(lambda x: x[0] if x else np.nan)\n",
    "            raw_df[\"d_lon\"] = coords.apply(lambda x: x[1] if x else np.nan)\n",
    "            raw_df = raw_df.dropna(subset=[\"d_lat\", \"d_lon\"]).reset_index(drop=True)\n",
    "\n",
    "            if raw_df.empty:\n",
    "                base_out = pd.DataFrame({\n",
    "                    \"point_id\": pts[\"point_id\"],\n",
    "                    \"disruption_count\": 0,\n",
    "                    \"is_works\": 0,\n",
    "                    \"is_incident\": 0,\n",
    "                    \"is_active\": 0,\n",
    "                    \"max_ordinal\": 0,\n",
    "                })\n",
    "            else:\n",
    "                # Cross join disruptions x points (works well for ~50-200 points)\n",
    "                cross = pts.rename(columns={\"latitude\": \"p_lat\", \"longitude\": \"p_lon\"}).copy()\n",
    "                cross[\"key\"] = 1\n",
    "                raw_df[\"key\"] = 1\n",
    "                cross = cross.merge(raw_df, on=\"key\").drop(columns=[\"key\"])\n",
    "\n",
    "                # Distance filter\n",
    "                cross[\"dist_km\"] = haversine_km(cross[\"p_lat\"], cross[\"p_lon\"], cross[\"d_lat\"], cross[\"d_lon\"])\n",
    "                cross = cross[cross[\"dist_km\"] <= RADIUS_KM].copy()\n",
    "\n",
    "                if cross.empty:\n",
    "                    base_out = pd.DataFrame({\n",
    "                        \"point_id\": pts[\"point_id\"],\n",
    "                        \"disruption_count\": 0,\n",
    "                        \"is_works\": 0,\n",
    "                        \"is_incident\": 0,\n",
    "                        \"is_active\": 0,\n",
    "                        \"max_ordinal\": 0,\n",
    "                    })\n",
    "                else:\n",
    "                    cross[\"disruption_count\"] = 1\n",
    "                    cross[\"is_works\"] = (cross.get(\"category\").astype(str) == \"Works\").astype(int)\n",
    "                    cross[\"is_incident\"] = (cross.get(\"category\").astype(str) == \"Incident\").astype(int)\n",
    "                    cross[\"is_active\"] = (cross.get(\"status\").astype(str).str.lower() == \"active\").astype(int)\n",
    "                    cross[\"max_ordinal\"] = pd.to_numeric(cross.get(\"ordinal\"), errors=\"coerce\").fillna(0)\n",
    "\n",
    "                    base_out = (\n",
    "                        cross.groupby(\"point_id\", as_index=False)\n",
    "                        .agg(\n",
    "                            disruption_count=(\"disruption_count\", \"sum\"),\n",
    "                            is_works=(\"is_works\", \"max\"),\n",
    "                            is_incident=(\"is_incident\", \"max\"),\n",
    "                            is_active=(\"is_active\", \"max\"),\n",
    "                            max_ordinal=(\"max_ordinal\", \"max\"),\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                    # Ensure all points exist (fill zeros)\n",
    "                    base_out = base_out.merge(pd.DataFrame({\"point_id\": pts[\"point_id\"]}), on=\"point_id\", how=\"right\")\n",
    "                    base_out = base_out.fillna(0)\n",
    "\n",
    "        # Replicate to 10-min timestamps\n",
    "        out_list = []\n",
    "        for ts_10m in ts_10m_list:\n",
    "            tmp = base_out.copy()\n",
    "            tmp[\"ts_10m\"] = ts_10m\n",
    "            out_list.append(tmp)\n",
    "\n",
    "        out = pd.concat(out_list, ignore_index=True)\n",
    "\n",
    "        # Type fixes for bigint columns\n",
    "        for c in [\"disruption_count\", \"is_works\", \"is_incident\", \"is_active\", \"max_ordinal\"]:\n",
    "            if c in out.columns:\n",
    "                out[c] = pd.to_numeric(out[c], errors=\"coerce\").fillna(0).astype(\"int64\")\n",
    "\n",
    "        out[\"point_id\"] = out[\"point_id\"].astype(str)\n",
    "        out[\"ts_10m\"] = pd.to_datetime(out[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "        # Align to schema (exact order)\n",
    "        out = out[[\n",
    "            \"point_id\", \"ts_10m\",\n",
    "            \"disruption_count\", \"is_active\", \"is_incident\", \"is_works\", \"max_ordinal\"\n",
    "        ]]\n",
    "\n",
    "        print(\"TfL 10-min rows to upsert:\", len(out))\n",
    "        tfl_10m_fg.insert(out, write_options={\"upsert\": True})\n",
    "        print(\"Upserted TfL 10-min rows for hour:\", hour_start)\n",
    "else:\n",
    "    print(\"Skipping TfL update (UPDATE_TFL=0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca6caf",
   "metadata": {},
   "source": [
    "## 9) Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
