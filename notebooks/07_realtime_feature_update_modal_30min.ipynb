{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed3b9a4",
   "metadata": {},
   "source": [
    "# 07 — Realtime Feature Update Job (Modal, every 30 minutes)\n",
    "\n",
    "This notebook is designed to run **headlessly** (e.g., on Modal) every 30 minutes.\n",
    "\n",
    "It:\n",
    "1. Logs into Hopsworks\n",
    "2. Loads monitoring points from metadata Feature Group\n",
    "3. Pulls **current** TomTom traffic data for each point\n",
    "4. Inserts raw rows into `traffic_flow_fg` (10-minute bucket `ts_10m`)\n",
    "5. Computes the **same engineered features** as your historical backfill notebook and inserts them into `traffic_temporal_fg`\n",
    "6. Updates hourly **weather** features from Open-Meteo and hourly **TfL disruptions** features (optional)\n",
    "\n",
    "### Notes on consistency\n",
    "Your historical engineering uses rolling windows defined in **number of rows** (10-min buckets).\n",
    "If you only fetch TomTom every 30 minutes, you will have fewer observations and the *effective time span*\n",
    "of a “window=3” rolling feature will become ~90 minutes.\n",
    "\n",
    "If you need strict *time-based* window semantics (e.g., exactly 30/60/120 minutes), you should either:\n",
    "- collect every 10 minutes, OR\n",
    "- redefine rolling features in minutes and backfill them consistently.\n",
    "\n",
    "For now, this notebook reproduces the **same feature definitions** as in the historical code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd95ba23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# If you run this notebook outside an environment that already has dependencies,\n",
    "# uncomment the following line:\n",
    "# !pip install -U hopsworks pandas numpy requests\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import hopsworks\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c90069",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f893c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_ts: 2026-01-10 14:42:22.386980+00:00\n",
      "ts_10m: 2026-01-10 14:40:00+00:00\n",
      "weather_time_utc: 2026-01-10 14:00:00+00:00\n",
      "tfl_time_utc: 2026-01-10 14:00:00+00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n"
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Run time\n",
    "# -------------------------\n",
    "# For Modal/papermill runs you can pass RUN_TS_UTC=\"2026-01-09T12:30:00Z\"\n",
    "RUN_TS_UTC = os.getenv(\"RUN_TS_UTC\", \"\")\n",
    "if RUN_TS_UTC:\n",
    "    run_ts = pd.to_datetime(RUN_TS_UTC, utc=True, errors=\"raise\")\n",
    "else:\n",
    "    run_ts = pd.Timestamp.now(tz=\"UTC\")\n",
    "\n",
    "# Use the same 10-minute bucketing as your raw Feature Group\n",
    "ts_10m = run_ts.floor(\"10min\")\n",
    "\n",
    "# Hourly buckets for weather and TfL\n",
    "weather_time_utc = ts_10m.floor(\"H\")\n",
    "tfl_time_utc = ts_10m.floor(\"H\")\n",
    "\n",
    "print(\"run_ts:\", run_ts)\n",
    "print(\"ts_10m:\", ts_10m)\n",
    "print(\"weather_time_utc:\", weather_time_utc)\n",
    "print(\"tfl_time_utc:\", tfl_time_utc)\n",
    "\n",
    "# -------------------------\n",
    "# Hopsworks connection\n",
    "# -------------------------\n",
    "HOPSWORKS_HOST = os.getenv(\"HOPSWORKS_HOST\", \"\")\n",
    "HOPSWORKS_PROJECT = os.getenv(\"HOPSWORKS_PROJECT\", \"\")\n",
    "HOPSWORKS_API_KEY = os.getenv(\"HOPSWORKS_API_KEY\", \"\")\n",
    "\n",
    "# -------------------------\n",
    "# API Keys\n",
    "# -------------------------\n",
    "TOMTOM_API_KEY = os.getenv(\"TOMTOM_API_KEY\", \"\")\n",
    "TFL_APP_ID = os.getenv(\"TFL_APP_ID\", \"\")\n",
    "TFL_APP_KEY = os.getenv(\"TFL_APP_KEY\", \"\")\n",
    "\n",
    "# -------------------------\n",
    "# Feature Groups (defaults aligned with your existing notebooks)\n",
    "# -------------------------\n",
    "RAW_FG_NAME = os.getenv(\"RAW_FG_NAME\", \"traffic_flow_fg\")\n",
    "RAW_FG_VERSION = int(os.getenv(\"RAW_FG_VERSION\", \"1\"))\n",
    "\n",
    "ENGINEERED_FG_NAME = os.getenv(\"ENGINEERED_FG_NAME\", \"traffic_temporal_fg\")\n",
    "ENGINEERED_FG_VERSION = int(os.getenv(\"ENGINEERED_FG_VERSION\", \"1\"))\n",
    "\n",
    "METADATA_FG_NAME = os.getenv(\"METADATA_FG_NAME\", \"traffic_points_metadata\")\n",
    "METADATA_FG_VERSION = int(os.getenv(\"METADATA_FG_VERSION\", \"1\"))\n",
    "\n",
    "WEATHER_FG_NAME = os.getenv(\"WEATHER_FG_NAME\", \"weather_hourly_fg\")\n",
    "WEATHER_FG_VERSION = int(os.getenv(\"WEATHER_FG_VERSION\", \"1\"))\n",
    "\n",
    "TFL_FG_NAME = os.getenv(\"TFL_FG_NAME\", \"tfl_disruptions_hourly_fg\")\n",
    "TFL_FG_VERSION = int(os.getenv(\"TFL_FG_VERSION\", \"1\"))\n",
    "\n",
    "# -------------------------\n",
    "# Feature engineering params (match your backfill defaults)\n",
    "# -------------------------\n",
    "LOW_CONF_THRESHOLD = float(os.getenv(\"LOW_CONF_THRESHOLD\", \"0.7\"))\n",
    "ROLL_WINDOWS = [3, 6, 12]  # designed for 10-min buckets\n",
    "\n",
    "# -------------------------\n",
    "# Controls\n",
    "# -------------------------\n",
    "INSERT_RAW = os.getenv(\"INSERT_RAW\", \"1\") == \"1\"\n",
    "INSERT_ENGINEERED = os.getenv(\"INSERT_ENGINEERED\", \"1\") == \"1\"\n",
    "UPDATE_WEATHER = os.getenv(\"UPDATE_WEATHER\", \"1\") == \"1\"\n",
    "UPDATE_TFL = os.getenv(\"UPDATE_TFL\", \"1\") == \"1\"\n",
    "\n",
    "LOOKBACK_HOURS = int(os.getenv(\"LOOKBACK_HOURS\", \"24\"))  # used to compute rolling features reliably\n",
    "\n",
    "assert TOMTOM_API_KEY, \"Missing TOMTOM_API_KEY env var\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab3bfb3",
   "metadata": {},
   "source": [
    "## 2) Login to Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d33ead12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-10 15:42:26,044 INFO: Closing external client and cleaning up certificates.\n",
      "2026-01-10 15:42:26,064 INFO: Connection closed.\n",
      "2026-01-10 15:42:26,066 INFO: Initializing external client\n",
      "2026-01-10 15:42:26,066 INFO: Base URL: https://eu-west.cloud.hopsworks.ai:443\n",
      "2026-01-10 15:42:27,295 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://eu-west.cloud.hopsworks.ai:443/p/3209\n",
      "Connected. Feature store: london_traffic_featurestore\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login(\n",
    "    host=\"eu-west.cloud.hopsworks.ai\",\n",
    "    project=\"London_traffic\"\n",
    ")\n",
    "fs = project.get_feature_store()\n",
    "raw_fg = fs.get_feature_group(name=RAW_FG_NAME, version=RAW_FG_VERSION)\n",
    "engineered_fg = fs.get_feature_group(name=ENGINEERED_FG_NAME, version=ENGINEERED_FG_VERSION)\n",
    "meta_fg = fs.get_feature_group(name=METADATA_FG_NAME, version=METADATA_FG_VERSION)\n",
    "\n",
    "print(\"Connected. Feature store:\", fs.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28042fd5",
   "metadata": {},
   "source": [
    "## 3) Load monitoring points (metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be267348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.10s) \n",
      "Loaded points: 200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8618</td>\n",
       "      <td>51.509307</td>\n",
       "      <td>-0.084878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16756</td>\n",
       "      <td>51.383489</td>\n",
       "      <td>-0.105944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38815</td>\n",
       "      <td>51.393451</td>\n",
       "      <td>0.029589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6458</td>\n",
       "      <td>51.573103</td>\n",
       "      <td>-0.212077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38022</td>\n",
       "      <td>51.589213</td>\n",
       "      <td>0.270734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  point_id   latitude  longitude\n",
       "0     8618  51.509307  -0.084878\n",
       "1    16756  51.383489  -0.105944\n",
       "2    38815  51.393451   0.029589\n",
       "3     6458  51.573103  -0.212077\n",
       "4    38022  51.589213   0.270734"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df = meta_fg.read()\n",
    "required = {\"point_id\", \"latitude\", \"longitude\"}\n",
    "missing = required - set(meta_df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Metadata FG missing required columns: {missing}\")\n",
    "\n",
    "points = meta_df[[\"point_id\", \"latitude\", \"longitude\"]].copy()\n",
    "points[\"point_id\"] = points[\"point_id\"].astype(str)\n",
    "points[\"latitude\"] = pd.to_numeric(points[\"latitude\"], errors=\"coerce\")\n",
    "points[\"longitude\"] = pd.to_numeric(points[\"longitude\"], errors=\"coerce\")\n",
    "points = points.dropna(subset=[\"point_id\", \"latitude\", \"longitude\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Loaded points:\", len(points))\n",
    "points.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedf7fd0",
   "metadata": {},
   "source": [
    "## 4) Fetch TomTom Flow data for all points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ec4c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw tick df shape: (50, 12) errors: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_id</th>\n",
       "      <th>timestamp_utc</th>\n",
       "      <th>ts_10m</th>\n",
       "      <th>current_speed</th>\n",
       "      <th>free_flow_speed</th>\n",
       "      <th>frc</th>\n",
       "      <th>current_travel_time</th>\n",
       "      <th>free_flow_travel_time</th>\n",
       "      <th>confidence</th>\n",
       "      <th>road_closure</th>\n",
       "      <th>speed_ratio</th>\n",
       "      <th>delay_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8618</td>\n",
       "      <td>2026-01-10 14:42:22.386980+00:00</td>\n",
       "      <td>2026-01-10 14:40:00+00:00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>FRC2</td>\n",
       "      <td>572.0</td>\n",
       "      <td>454.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16756</td>\n",
       "      <td>2026-01-10 14:42:22.386980+00:00</td>\n",
       "      <td>2026-01-10 14:40:00+00:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>FRC3</td>\n",
       "      <td>223.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38815</td>\n",
       "      <td>2026-01-10 14:42:22.386980+00:00</td>\n",
       "      <td>2026-01-10 14:40:00+00:00</td>\n",
       "      <td>33.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>FRC3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6458</td>\n",
       "      <td>2026-01-10 14:42:22.386980+00:00</td>\n",
       "      <td>2026-01-10 14:40:00+00:00</td>\n",
       "      <td>48.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>FRC2</td>\n",
       "      <td>176.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38022</td>\n",
       "      <td>2026-01-10 14:42:22.386980+00:00</td>\n",
       "      <td>2026-01-10 14:40:00+00:00</td>\n",
       "      <td>108.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>FRC0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  point_id                    timestamp_utc                    ts_10m  current_speed  free_flow_speed   frc  current_travel_time  free_flow_travel_time  confidence  road_closure  speed_ratio  \\\n",
       "0     8618 2026-01-10 14:42:22.386980+00:00 2026-01-10 14:40:00+00:00           23.0             29.0  FRC2                572.0                  454.0         1.0         False     0.793103   \n",
       "1    16756 2026-01-10 14:42:22.386980+00:00 2026-01-10 14:40:00+00:00            4.0             18.0  FRC3                223.0                   49.0         1.0         False     0.222222   \n",
       "2    38815 2026-01-10 14:42:22.386980+00:00 2026-01-10 14:40:00+00:00           33.0             44.0  FRC3                 49.0                   37.0         1.0         False     0.750000   \n",
       "3     6458 2026-01-10 14:42:22.386980+00:00 2026-01-10 14:40:00+00:00           48.0             58.0  FRC2                176.0                  146.0         1.0         False     0.827586   \n",
       "4    38022 2026-01-10 14:42:22.386980+00:00 2026-01-10 14:40:00+00:00          108.0            108.0  FRC0                132.0                  132.0         1.0         False     1.000000   \n",
       "\n",
       "   delay_seconds  \n",
       "0          118.0  \n",
       "1          174.0  \n",
       "2           12.0  \n",
       "3           30.0  \n",
       "4            0.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_POINTS = 50\n",
    "\n",
    "points_subset = (\n",
    "    points\n",
    "    .dropna(subset=[\"latitude\", \"longitude\"])\n",
    "    .head(MAX_POINTS)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "def fetch_tomtom_flow(lat: float, lon: float, *, zoom: int = 10, unit: str = \"kmph\") -> Dict:\n",
    "    # TomTom Flow Segment Data API (v4)\n",
    "    url = f\"https://api.tomtom.com/traffic/services/4/flowSegmentData/absolute/{zoom}/json\"\n",
    "    params = {\n",
    "        \"key\": TOMTOM_API_KEY,\n",
    "        \"point\": f\"{lat},{lon}\",\n",
    "        \"unit\": unit,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def parse_tomtom_payload(payload: Dict) -> Dict:\n",
    "    fsd = payload.get(\"flowSegmentData\", {}) if isinstance(payload, dict) else {}\n",
    "    frc = fsd.get(\"frc\")\n",
    "    current_speed = fsd.get(\"currentSpeed\")\n",
    "    free_flow_speed = fsd.get(\"freeFlowSpeed\")\n",
    "    current_tt = fsd.get(\"currentTravelTime\")\n",
    "    free_flow_tt = fsd.get(\"freeFlowTravelTime\")\n",
    "    confidence = fsd.get(\"confidence\")\n",
    "    road_closure = fsd.get(\"roadClosure\")\n",
    "\n",
    "    # Derived\n",
    "    speed_ratio = None\n",
    "    delay_seconds = None\n",
    "    try:\n",
    "        if current_speed is not None and free_flow_speed:\n",
    "            speed_ratio = float(current_speed) / float(free_flow_speed)\n",
    "    except Exception:\n",
    "        speed_ratio = None\n",
    "\n",
    "    try:\n",
    "        if current_tt is not None and free_flow_tt is not None:\n",
    "            delay_seconds = float(current_tt) - float(free_flow_tt)\n",
    "    except Exception:\n",
    "        delay_seconds = None\n",
    "\n",
    "    return {\n",
    "        \"current_speed\": float(current_speed) if current_speed is not None else np.nan,\n",
    "        \"free_flow_speed\": float(free_flow_speed) if free_flow_speed is not None else np.nan,\n",
    "        \"frc\": str(frc) if frc is not None else np.nan,\n",
    "        \"current_travel_time\": float(current_tt) if current_tt is not None else np.nan,\n",
    "        \"free_flow_travel_time\": float(free_flow_tt) if free_flow_tt is not None else np.nan,\n",
    "        \"confidence\": float(confidence) if confidence is not None else np.nan,\n",
    "        \"road_closure\": bool(road_closure) if road_closure is not None else False,\n",
    "        \"speed_ratio\": float(speed_ratio) if speed_ratio is not None else np.nan,\n",
    "        \"delay_seconds\": float(delay_seconds) if delay_seconds is not None else np.nan,\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "errors = 0\n",
    "\n",
    "for _, p in points_subset.iterrows():\n",
    "    pid = str(p[\"point_id\"])\n",
    "    lat = float(p[\"latitude\"])\n",
    "    lon = float(p[\"longitude\"])\n",
    "    try:\n",
    "        payload = fetch_tomtom_flow(lat, lon)\n",
    "        parsed = parse_tomtom_payload(payload)\n",
    "        rows.append({\n",
    "            \"point_id\": pid,\n",
    "            \"timestamp_utc\": run_ts,\n",
    "            \"ts_10m\": ts_10m,\n",
    "            **parsed,\n",
    "        })\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        rows.append({\n",
    "            \"point_id\": pid,\n",
    "            \"timestamp_utc\": run_ts,\n",
    "            \"ts_10m\": ts_10m,\n",
    "            \"error\": str(e),\n",
    "        })\n",
    "\n",
    "raw_tick_df = pd.DataFrame(rows)\n",
    "\n",
    "print(\"Raw tick df shape:\", raw_tick_df.shape, \"errors:\", errors)\n",
    "\n",
    "raw_tick_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f136b",
   "metadata": {},
   "source": [
    "## 5) Insert RAW tick into `traffic_flow_fg` (optional, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f063d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (2.09s) \n",
      "Raw rows to insert: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 50/50 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: traffic_flow_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://eu-west.cloud.hopsworks.ai:443/p/3209/jobs/named/traffic_flow_fg_1_offline_fg_materialization/executions\n",
      "Inserted raw tick.\n"
     ]
    }
   ],
   "source": [
    "if INSERT_RAW:\n",
    "    # Keep only columns expected by your raw FG schema. If schema differs, adjust here.\n",
    "    # We also ensure road_closure is numpy bool (not pandas nullable boolean).\n",
    "    keep_cols = [\n",
    "        \"point_id\", \"timestamp_utc\", \"ts_10m\", \"frc\",\n",
    "        \"current_speed\", \"free_flow_speed\", \"current_travel_time\", \"free_flow_travel_time\",\n",
    "        \"confidence\", \"road_closure\", \"speed_ratio\", \"delay_seconds\"\n",
    "    ]\n",
    "    missing = [c for c in keep_cols if c not in raw_tick_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"raw_tick_df missing columns: {missing}\")\n",
    "\n",
    "    df_ins = raw_tick_df[keep_cols].copy()\n",
    "    df_ins[\"point_id\"] = df_ins[\"point_id\"].astype(str)\n",
    "    df_ins[\"timestamp_utc\"] = pd.to_datetime(df_ins[\"timestamp_utc\"], utc=True)\n",
    "    df_ins[\"ts_10m\"] = pd.to_datetime(df_ins[\"ts_10m\"], utc=True)\n",
    "\n",
    "    # Fix Avro bool edge case\n",
    "    df_ins[\"road_closure\"] = df_ins[\"road_closure\"].fillna(False).astype(bool)\n",
    "\n",
    "    # Deduplicate on PK\n",
    "    df_ins = df_ins.drop_duplicates(subset=[\"point_id\", \"ts_10m\"], keep=\"last\")\n",
    "\n",
    "    # Avoid inserting duplicates if rerun\n",
    "    existing_raw = raw_fg.read()\n",
    "    existing_raw[\"ts_10m\"] = pd.to_datetime(existing_raw[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "    existing_keys = set(zip(existing_raw[\"point_id\"].astype(str), existing_raw[\"ts_10m\"]))\n",
    "    df_ins[\"__key\"] = list(zip(df_ins[\"point_id\"].astype(str), df_ins[\"ts_10m\"]))\n",
    "    df_ins = df_ins[~df_ins[\"__key\"].isin(existing_keys)].drop(columns=\"__key\")\n",
    "\n",
    "    print(\"Raw rows to insert:\", len(df_ins))\n",
    "    if len(df_ins) > 0:\n",
    "        raw_fg.insert(df_ins)\n",
    "        print(\"Inserted raw tick.\")\n",
    "    else:\n",
    "        print(\"No new raw rows (already present).\")\n",
    "else:\n",
    "    print(\"Skipping RAW insert (INSERT_RAW=0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa6522",
   "metadata": {},
   "source": [
    "## 6) Compute engineered features for this tick and insert into `traffic_temporal_fg`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1198bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.37s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.72s) \n",
      "Engineered rows to insert: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 50/50 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: traffic_temporal_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://eu-west.cloud.hopsworks.ai:443/p/3209/jobs/named/traffic_temporal_fg_1_offline_fg_materialization/executions\n",
      "Inserted engineered tick.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1768058159.066|FAIL|rdkafka#producer-3| [thrd:ssl://57.130.28.56:9093/0]: ssl://57.130.28.56:9093/0: Disconnected: SSL connection closed by peer (after 50071ms in state UP)\n"
     ]
    }
   ],
   "source": [
    "def time_interval(hour: int) -> str:\n",
    "    if 7 <= hour < 10:\n",
    "        return \"morning_peak\"\n",
    "    elif 10 <= hour < 16:\n",
    "        return \"midday\"\n",
    "    elif 16 <= hour < 19:\n",
    "        return \"evening_peak\"\n",
    "    else:\n",
    "        return \"night\"\n",
    "\n",
    "def build_engineered_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "\n",
    "    # Ensure types\n",
    "    df[\"point_id\"] = df[\"point_id\"].astype(str)\n",
    "    df[\"ts_10m\"] = pd.to_datetime(df[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "    df[\"timestamp_utc\"] = pd.to_datetime(df.get(\"timestamp_utc\", df[\"ts_10m\"]), utc=True, errors=\"coerce\")\n",
    "\n",
    "    # Sort + dedupe\n",
    "    df = df.dropna(subset=[\"point_id\", \"ts_10m\"])\n",
    "    df = df.sort_values([\"point_id\", \"ts_10m\"]).drop_duplicates(subset=[\"point_id\", \"ts_10m\"], keep=\"last\")\n",
    "\n",
    "    # Temporal features (same as your backfill)\n",
    "    df[\"day_of_week\"] = df[\"ts_10m\"].dt.weekday\n",
    "    df[\"is_weekend\"] = df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "\n",
    "    df[\"hour\"] = df[\"ts_10m\"].dt.hour\n",
    "    df[\"minute\"] = df[\"ts_10m\"].dt.minute\n",
    "\n",
    "    df[\"is_rush_hour\"] = ((df[\"hour\"].between(7, 9)) | (df[\"hour\"].between(16, 18))).astype(int)\n",
    "\n",
    "    df[\"time_interval\"] = df[\"hour\"].apply(time_interval)\n",
    "    dummies = pd.get_dummies(df[\"time_interval\"], prefix=\"ti\")\n",
    "\n",
    "    # Ensure stable dummy columns\n",
    "    for col in [\"ti_morning_peak\", \"ti_midday\", \"ti_evening_peak\", \"ti_night\"]:\n",
    "        if col not in dummies.columns:\n",
    "            dummies[col] = 0\n",
    "\n",
    "    df = pd.concat([df.drop(columns=[\"time_interval\"]), dummies[[\"ti_morning_peak\",\"ti_midday\",\"ti_evening_peak\",\"ti_night\"]]], axis=1)\n",
    "    #change to bool\n",
    "    df[\"ti_morning_peak\"] = df[\"ti_morning_peak\"].astype(bool)\n",
    "    df[\"ti_midday\"] = df[\"ti_midday\"].astype(bool)\n",
    "    df[\"ti_evening_peak\"] = df[\"ti_evening_peak\"].astype(bool)\n",
    "    df[\"ti_night\"] = df[\"ti_night\"].astype(bool)\n",
    "    # Traffic features (same as your backfill)\n",
    "    eps = 1e-6\n",
    "    df[\"speed_diff\"] = df[\"free_flow_speed\"] - df[\"current_speed\"]\n",
    "    df[\"travel_time_ratio\"] = df[\"current_travel_time\"] / (df[\"free_flow_travel_time\"] + eps)\n",
    "    df[\"low_confidence_flag\"] = (df[\"confidence\"] < LOW_CONF_THRESHOLD).astype(int)\n",
    "    df[\"travel_time_ratio\"] = df[\"travel_time_ratio\"].clip(lower=0, upper=10)\n",
    "\n",
    "    # Rolling features over rows (same as your backfill)\n",
    "    for w in ROLL_WINDOWS:\n",
    "        df[f\"speed_roll_mean_{w}\"] = (\n",
    "            df.groupby(\"point_id\")[\"current_speed\"]\n",
    "              .rolling(window=w, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "        df[f\"speed_roll_std_{w}\"] = (\n",
    "            df.groupby(\"point_id\")[\"current_speed\"]\n",
    "              .rolling(window=w, min_periods=1)\n",
    "              .std()\n",
    "              .reset_index(level=0, drop=True)\n",
    "              .fillna(0.0)\n",
    "        )\n",
    "        df[f\"delay_roll_mean_{w}\"] = (\n",
    "            df.groupby(\"point_id\")[\"delay_seconds\"]\n",
    "              .rolling(window=w, min_periods=1)\n",
    "              .mean()\n",
    "              .reset_index(level=0, drop=True)\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "if INSERT_ENGINEERED:\n",
    "    # Read history for lookback\n",
    "    hist = raw_fg.read()\n",
    "    hist[\"ts_10m\"] = pd.to_datetime(hist[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "    hist[\"timestamp_utc\"] = pd.to_datetime(hist.get(\"timestamp_utc\", hist[\"ts_10m\"]), utc=True, errors=\"coerce\")\n",
    "    hist[\"point_id\"] = hist[\"point_id\"].astype(str)\n",
    "\n",
    "    lookback_start = ts_10m - pd.Timedelta(hours=LOOKBACK_HOURS)\n",
    "    hist = hist[hist[\"ts_10m\"] >= lookback_start].copy()\n",
    "\n",
    "    # Append current tick \n",
    "    current_raw = raw_tick_df.copy()\n",
    "    # Keep only required raw columns for feature engineering\n",
    "    needed = [\n",
    "        \"point_id\", \"timestamp_utc\", \"ts_10m\", \"frc\",\n",
    "        \"current_speed\", \"free_flow_speed\", \"current_travel_time\", \"free_flow_travel_time\",\n",
    "        \"confidence\", \"road_closure\", \"speed_ratio\", \"delay_seconds\"\n",
    "    ]\n",
    "    current_raw = current_raw[needed].copy()\n",
    "    current_raw[\"timestamp_utc\"] = pd.to_datetime(current_raw[\"timestamp_utc\"], utc=True)\n",
    "    current_raw[\"ts_10m\"] = pd.to_datetime(current_raw[\"ts_10m\"], utc=True)\n",
    "\n",
    "    combined = pd.concat([hist, current_raw], ignore_index=True)\n",
    "    combined = combined.drop_duplicates(subset=[\"point_id\", \"ts_10m\"], keep=\"last\")\n",
    "    combined = combined.sort_values([\"point_id\", \"ts_10m\"]).reset_index(drop=True)\n",
    "\n",
    "    eng_all = build_engineered_features(combined)\n",
    "\n",
    "    # Keep only current tick rows\n",
    "    eng_tick = eng_all[eng_all[\"ts_10m\"] == ts_10m].copy()\n",
    "    eng_tick = eng_tick.drop_duplicates(subset=[\"point_id\", \"ts_10m\"], keep=\"last\")\n",
    "\n",
    "    # Align columns to FG schema (important!)\n",
    "    schema_cols = [f.name for f in engineered_fg.features]\n",
    "    for c in schema_cols:\n",
    "        if c not in eng_tick.columns:\n",
    "            eng_tick[c] = np.nan\n",
    "\n",
    "    eng_tick = eng_tick[schema_cols].copy()\n",
    "\n",
    "    # Avoid duplicates if rerun\n",
    "    existing_eng = engineered_fg.read()\n",
    "    existing_eng[\"ts_10m\"] = pd.to_datetime(existing_eng[\"ts_10m\"], utc=True, errors=\"coerce\")\n",
    "    existing_keys = set(zip(existing_eng[\"point_id\"].astype(str), existing_eng[\"ts_10m\"]))\n",
    "    eng_tick[\"__key\"] = list(zip(eng_tick[\"point_id\"].astype(str), eng_tick[\"ts_10m\"]))\n",
    "    eng_tick = eng_tick[~eng_tick[\"__key\"].isin(existing_keys)].drop(columns=\"__key\")\n",
    "\n",
    "    print(\"Engineered rows to insert:\", len(eng_tick))\n",
    "    if len(eng_tick) > 0:\n",
    "        engineered_fg.insert(eng_tick)\n",
    "        print(\"Inserted engineered tick.\")\n",
    "    else:\n",
    "        print(\"No new engineered rows (already present).\")\n",
    "else:\n",
    "    print(\"Skipping engineered insert (INSERT_ENGINEERED=0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01891f9",
   "metadata": {},
   "source": [
    "## 7) Update hourly Weather features from Open-Meteo (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "686aa663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1768059547.378|FAIL|rdkafka#producer-9| [thrd:ssl://57.130.18.242:9093/2]: ssl://57.130.18.242:9093/2: Disconnected: SSL connection closed by peer (after 50035ms in state UP, 1 identical error(s) suppressed)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather rows to insert: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading Dataframe: 100.00% |██████████| Rows 200/200 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: weather_hourly_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://eu-west.cloud.hopsworks.ai:443/p/3209/jobs/named/weather_hourly_fg_1_offline_fg_materialization/executions\n",
      "Upserted weather hour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1768059573.849|FAIL|rdkafka#producer-7| [thrd:ssl://57.130.28.56:9093/0]: ssl://57.130.28.56:9093/0: Disconnected: SSL connection closed by peer (after 50039ms in state UP, 1 identical error(s) suppressed)\n"
     ]
    }
   ],
   "source": [
    "def fetch_openmeteo_hourly(lat: float, lon: float, hour_utc: pd.Timestamp) -> Dict[str, float]:\n",
    "    # Forecast API is better for \"now\" \n",
    "    url = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    hourly_vars = [\n",
    "        \"temperature_2m\", \"precipitation\", \"rain\", \"snowfall\",\n",
    "    ]\n",
    "    params = {\n",
    "        \"latitude\": lat,\n",
    "        \"longitude\": lon,\n",
    "        \"hourly\": \",\".join(hourly_vars),\n",
    "        \"timezone\": \"UTC\",\n",
    "        \"past_days\": 1,\n",
    "    }\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    js = r.json()\n",
    "\n",
    "    times = js.get(\"hourly\", {}).get(\"time\", [])\n",
    "    if not times:\n",
    "        raise RuntimeError(\"Open-Meteo returned no hourly times\")\n",
    "\n",
    "    t = pd.to_datetime(times, utc=True, errors=\"coerce\")\n",
    "    idx = int(np.argmin(np.abs((t - hour_utc).total_seconds())))\n",
    "\n",
    "    out = {}\n",
    "    for v in hourly_vars:\n",
    "        arr = js.get(\"hourly\", {}).get(v, [])\n",
    "        out[v] = float(arr[idx]) if idx < len(arr) and arr[idx] is not None else np.nan\n",
    "    return out\n",
    "\n",
    "if UPDATE_WEATHER:\n",
    "    weather_fg = fs.get_feature_group(name=WEATHER_FG_NAME, version=WEATHER_FG_VERSION)\n",
    "\n",
    "    # Load existing keys for this hour to avoid duplicates\n",
    "    existing_weather = weather_fg.read()\n",
    "    if \"weather_time_utc\" in existing_weather.columns:\n",
    "        existing_weather[\"weather_time_utc\"] = pd.to_datetime(existing_weather[\"weather_time_utc\"], utc=True, errors=\"coerce\")\n",
    "        existing_weather[\"point_id\"] = existing_weather[\"point_id\"].astype(str)\n",
    "        existing_keys = set(zip(existing_weather[\"point_id\"], existing_weather[\"weather_time_utc\"]))\n",
    "    else:\n",
    "        existing_keys = set()\n",
    "\n",
    "    weather_rows = []\n",
    "    for _, p in points.iterrows():\n",
    "        pid = str(p[\"point_id\"])\n",
    "        lat = float(p[\"latitude\"])\n",
    "        lon = float(p[\"longitude\"])\n",
    "        key = (pid, weather_time_utc)\n",
    "        if key in existing_keys:\n",
    "            continue\n",
    "        try:\n",
    "            vals = fetch_openmeteo_hourly(lat, lon, weather_time_utc)\n",
    "            weather_rows.append({\"point_id\": pid, \"weather_time_utc\": weather_time_utc, **vals})\n",
    "        except Exception as e:\n",
    "            weather_rows.append({\"point_id\": pid, \"weather_time_utc\": weather_time_utc, \"error\": str(e)})\n",
    "\n",
    "    weather_df_new = pd.DataFrame(weather_rows)\n",
    "\n",
    "    if len(weather_df_new) > 0:\n",
    "        # Drop error column if schema does not include it\n",
    "        schema_cols = [f.name for f in weather_fg.features]\n",
    "        if \"error\" in weather_df_new.columns and \"error\" not in schema_cols:\n",
    "            weather_df_new = weather_df_new.drop(columns=[\"error\"])\n",
    "\n",
    "        for c in schema_cols:\n",
    "            if c not in weather_df_new.columns:\n",
    "                weather_df_new[c] = np.nan\n",
    "        weather_df_new = weather_df_new[schema_cols]\n",
    "\n",
    "        print(\"Weather rows to insert:\", len(weather_df_new))\n",
    "        weather_fg.insert(weather_df_new)\n",
    "        print(\"Inserted weather hour.\")\n",
    "    else:\n",
    "        print(\"No new weather rows for this hour (already present).\")\n",
    "else:\n",
    "    print(\"Skipping weather update (UPDATE_WEATHER=0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54ddc76",
   "metadata": {},
   "source": [
    "## 8) Update hourly TfL disruption features (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0a14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfL rows to upsert: 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading Dataframe: 100.00% |██████████| Rows 200/200 | Elapsed Time: 00:00 | Remaining Time: 00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching job: tfl_disruptions_hourly_fg_1_offline_fg_materialization\n",
      "Job started successfully, you can follow the progress at \n",
      "https://eu-west.cloud.hopsworks.ai:443/p/3209/jobs/named/tfl_disruptions_hourly_fg_1_offline_fg_materialization/executions\n",
      "Upserted TfL hour.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "%6|1768060404.812|FAIL|rdkafka#producer-13| [thrd:ssl://57.130.18.242:9093/2]: ssl://57.130.18.242:9093/2: Disconnected: SSL connection closed by peer (after 50142ms in state UP)\n",
      "%6|1768060456.010|FAIL|rdkafka#producer-13| [thrd:ssl://57.130.28.56:9093/0]: ssl://57.130.28.56:9093/0: Disconnected: SSL connection closed by peer (after 50149ms in state UP)\n",
      "%6|1768060507.045|FAIL|rdkafka#producer-13| [thrd:ssl://57.130.19.97:9093/1]: ssl://57.130.19.97:9093/1: Disconnected: SSL connection closed by peer (after 50040ms in state UP)\n",
      "%6|1768060557.274|FAIL|rdkafka#producer-13| [thrd:ssl://57.130.19.97:9093/1]: ssl://57.130.19.97:9093/1: Disconnected: SSL connection closed by peer (after 50073ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1768060607.506|FAIL|rdkafka#producer-13| [thrd:ssl://57.130.18.242:9093/2]: ssl://57.130.18.242:9093/2: Disconnected: SSL connection closed by peer (after 50079ms in state UP, 1 identical error(s) suppressed)\n",
      "%6|1768060703.531|FAIL|rdkafka#producer-13| [thrd:ssl://57.130.18.242:9093/2]: ssl://57.130.18.242:9093/2: Disconnected: SSL connection closed by peer (after 94858ms in state UP, 1 identical error(s) suppressed)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from typing import Optional\n",
    "\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R*c\n",
    "\n",
    "def extract_point_latlon(point_obj) -> Optional[tuple]:\n",
    "    if not isinstance(point_obj, dict):\n",
    "        return None\n",
    "    lat = point_obj.get(\"lat\", None)\n",
    "    lon = point_obj.get(\"lon\", None)\n",
    "    if lat is None or lon is None:\n",
    "        lat = point_obj.get(\"latitude\", None)\n",
    "        lon = point_obj.get(\"longitude\", None)\n",
    "    try:\n",
    "        if lat is None or lon is None:\n",
    "            return None\n",
    "        return (float(lat), float(lon))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fetch_tfl_active_road_disruptions() -> pd.DataFrame:\n",
    "    # Stable: returns current/active road disruptions\n",
    "    url = \"https://api.tfl.gov.uk/Road/all/Disruption\"\n",
    "    params = {\n",
    "        \"stripContent\": True,  # lighter payload\n",
    "        \"closures\": True,      # include closures if available\n",
    "    }\n",
    "    if TFL_APP_ID:\n",
    "        params[\"app_id\"] = TFL_APP_ID\n",
    "    if TFL_APP_KEY:\n",
    "        params[\"app_key\"] = TFL_APP_KEY\n",
    "\n",
    "    r = requests.get(url, params=params, timeout=60)\n",
    "    if r.status_code >= 400:\n",
    "        raise RuntimeError(f\"TfL {r.status_code}: {r.text[:500]}\")\n",
    "    r.raise_for_status()\n",
    "    raw = r.json()\n",
    "    return pd.DataFrame(raw if isinstance(raw, list) else [])\n",
    "\n",
    "def to_int64_nullable(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\").round().astype(\"Int64\")\n",
    "\n",
    "RADIUS_KM = float(os.getenv(\"TFL_RADIUS_KM\", \"0.5\"))\n",
    "\n",
    "if UPDATE_TFL:\n",
    "    if not TFL_APP_KEY and not TFL_APP_ID:\n",
    "        print(\"Skipping TfL update because TFL_APP_KEY/TFL_APP_ID are not set.\")\n",
    "    else:\n",
    "        tfl_fg = fs.get_feature_group(name=TFL_FG_NAME, version=TFL_FG_VERSION)\n",
    "\n",
    "        # IMPORTANT: create a row for every point for this hour (no need to read FG)\n",
    "        pts = points.copy()\n",
    "        pts[\"point_id\"] = pts[\"point_id\"].astype(str)\n",
    "        pts = pts.dropna(subset=[\"latitude\", \"longitude\"]).reset_index(drop=True)\n",
    "\n",
    "        # (optional) limit to your working 50 points\n",
    "        # pts = pts.head(50).reset_index(drop=True)\n",
    "\n",
    "        # Fetch current disruptions\n",
    "        raw_df = fetch_tfl_active_road_disruptions()\n",
    "\n",
    "        # Define hour window\n",
    "        hour_start = pd.to_datetime(tfl_time_utc, utc=True)\n",
    "        hour_end = hour_start + pd.Timedelta(hours=1)\n",
    "\n",
    "        # If nothing returned, just write zeros for all points\n",
    "        if raw_df.empty:\n",
    "            out = pd.DataFrame({\n",
    "                \"point_id\": pts[\"point_id\"],\n",
    "                \"tfl_time_utc\": hour_start,\n",
    "                \"disruption_count\": 0,\n",
    "                \"is_works\": 0,\n",
    "                \"is_incident\": 0,\n",
    "                \"is_active\": 0,\n",
    "                \"max_ordinal\": 0,\n",
    "            })\n",
    "        else:\n",
    "            # Parse disruption times (if present)\n",
    "            raw_df[\"start\"] = pd.to_datetime(raw_df.get(\"startDateTime\"), utc=True, errors=\"coerce\")\n",
    "            raw_df[\"end\"] = pd.to_datetime(raw_df.get(\"endDateTime\"), utc=True, errors=\"coerce\")\n",
    "            raw_df[\"end_filled\"] = raw_df[\"end\"]\n",
    "            raw_df.loc[raw_df[\"end_filled\"].isna(), \"end_filled\"] = raw_df[\"start\"] + pd.Timedelta(hours=1)\n",
    "\n",
    "            # Keep only disruptions overlapping the hour (if start is missing, keep it as \"maybe\")\n",
    "            raw_df = raw_df[\n",
    "                (raw_df[\"start\"].isna()) | ((raw_df[\"start\"] < hour_end) & (raw_df[\"end_filled\"] >= hour_start))\n",
    "            ].copy()\n",
    "\n",
    "            # Extract disruption point coords\n",
    "            coords = raw_df.get(\"point\", pd.Series([None]*len(raw_df))).apply(extract_point_latlon)\n",
    "            raw_df[\"d_lat\"] = coords.apply(lambda x: x[0] if x else np.nan)\n",
    "            raw_df[\"d_lon\"] = coords.apply(lambda x: x[1] if x else np.nan)\n",
    "            raw_df = raw_df.dropna(subset=[\"d_lat\", \"d_lon\"]).reset_index(drop=True)\n",
    "\n",
    "            if raw_df.empty:\n",
    "                out = pd.DataFrame({\n",
    "                    \"point_id\": pts[\"point_id\"],\n",
    "                    \"tfl_time_utc\": hour_start,\n",
    "                    \"disruption_count\": 0,\n",
    "                    \"is_works\": 0,\n",
    "                    \"is_incident\": 0,\n",
    "                    \"is_active\": 0,\n",
    "                    \"max_ordinal\": 0,\n",
    "                })\n",
    "            else:\n",
    "                # Cross join disruptions x points (small: ~50)\n",
    "                cross = pts.rename(columns={\"latitude\": \"p_lat\", \"longitude\": \"p_lon\"}).copy()\n",
    "                cross[\"key\"] = 1\n",
    "                raw_df[\"key\"] = 1\n",
    "                cross = cross.merge(raw_df, on=\"key\").drop(columns=[\"key\"])\n",
    "\n",
    "                # Distance filter\n",
    "                cross[\"dist_km\"] = haversine_km(cross[\"p_lat\"], cross[\"p_lon\"], cross[\"d_lat\"], cross[\"d_lon\"])\n",
    "                cross = cross[cross[\"dist_km\"] <= RADIUS_KM].copy()\n",
    "\n",
    "                if cross.empty:\n",
    "                    out = pd.DataFrame({\n",
    "                        \"point_id\": pts[\"point_id\"],\n",
    "                        \"tfl_time_utc\": hour_start,\n",
    "                        \"disruption_count\": 0,\n",
    "                        \"is_works\": 0,\n",
    "                        \"is_incident\": 0,\n",
    "                        \"is_active\": 0,\n",
    "                        \"max_ordinal\": 0,\n",
    "                    })\n",
    "                else:\n",
    "                    cross[\"disruption_count\"] = 1\n",
    "                    cross[\"is_works\"] = (cross.get(\"category\").astype(str) == \"Works\").astype(int)\n",
    "                    cross[\"is_incident\"] = (cross.get(\"category\").astype(str) == \"Incident\").astype(int)\n",
    "                    cross[\"is_active\"] = (cross.get(\"status\").astype(str).str.lower() == \"active\").astype(int)\n",
    "                    cross[\"max_ordinal\"] = pd.to_numeric(cross.get(\"ordinal\"), errors=\"coerce\").fillna(0)\n",
    "\n",
    "                    out = (\n",
    "                        cross.groupby(\"point_id\", as_index=False)\n",
    "                        .agg(\n",
    "                            disruption_count=(\"disruption_count\", \"sum\"),\n",
    "                            is_works=(\"is_works\", \"max\"),\n",
    "                            is_incident=(\"is_incident\", \"max\"),\n",
    "                            is_active=(\"is_active\", \"max\"),\n",
    "                            max_ordinal=(\"max_ordinal\", \"max\"),\n",
    "                        )\n",
    "                    )\n",
    "                    out[\"tfl_time_utc\"] = hour_start\n",
    "\n",
    "                    # Ensure all points exist (fill zeros)\n",
    "                    out = out.merge(pd.DataFrame({\"point_id\": pts[\"point_id\"]}), on=\"point_id\", how=\"right\")\n",
    "                    out = out.fillna(0)\n",
    "\n",
    "        # Align to schema\n",
    "        schema_cols = [f.name for f in tfl_fg.features]\n",
    "        for c in schema_cols:\n",
    "            if c not in out.columns:\n",
    "                out[c] = np.nan\n",
    "        out = out[schema_cols]\n",
    "\n",
    "        print(\"TfL rows to upsert:\", len(out))\n",
    "        tfl_fg.insert(out, write_options={\"upsert\": True})\n",
    "        print(\"Upserted TfL hour.\")\n",
    "else:\n",
    "    print(\"Skipping TfL update (UPDATE_TFL=0).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca6caf",
   "metadata": {},
   "source": [
    "## 9) Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
