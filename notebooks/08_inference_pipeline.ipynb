{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca483a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import hopsworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20094c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Hopsworks\n",
    "# =========================\n",
    "PROJECT_NAME = os.environ.get(\"HOPSWORKS_PROJECT_NAME\")\n",
    "HOPSWORKS_API_KEY = os.environ.get(\"HOPSWORKS_API_KEY\")\n",
    "\n",
    "# =========================\n",
    "# Feature Groups\n",
    "# =========================\n",
    "TRAFFIC_FG_NAME, TRAFFIC_FG_VER = \"traffic_temporal_fg\", 1\n",
    "WEATHER_FG_NAME, WEATHER_FG_VER = \"weather_10m_fg\", 1\n",
    "TFL_FG_NAME, TFL_FG_VER         = \"tfl_disruptions_10m_fg\", 1\n",
    "\n",
    "# =========================\n",
    "# Columns\n",
    "# =========================\n",
    "POINT_ID_COL = \"point_id\"\n",
    "\n",
    "# If your FG already has ts_10m use that; otherwise we create it from timestamp_utc\n",
    "TS_10M_COL = \"ts_10m\"\n",
    "RAW_TS_CANDIDATES = [\"timestamp_utc\", \"timestamp\", \"datetime\", \"time\"]\n",
    "\n",
    "# =========================\n",
    "# Model\n",
    "# =========================\n",
    "MODEL_NAME = \"traffic_speed_ratio_keras\"   \n",
    "MODEL_VERSION = None                      \n",
    "\n",
    "PRED_COL_30 = \"pred_speed_ratio_t+30\"\n",
    "PRED_COL_60 = \"pred_speed_ratio_t+60\"\n",
    "\n",
    "# =========================\n",
    "# Output\n",
    "# =========================\n",
    "N_POINTS = 50\n",
    "OUT_JSON_PATH = \"predictions_latest.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9df09e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 17:22:55,519 INFO: Closing external client and cleaning up certificates.\n",
      "2026-01-11 17:22:55,538 INFO: Connection closed.\n",
      "2026-01-11 17:22:55,540 INFO: Initializing external client\n",
      "2026-01-11 17:22:55,540 INFO: Base URL: https://eu-west.cloud.hopsworks.ai:443\n",
      "2026-01-11 17:22:56,803 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://eu-west.cloud.hopsworks.ai:443/p/3209\n"
     ]
    }
   ],
   "source": [
    "# 1. Login to Hopsworks\n",
    "project = hopsworks.login(\n",
    "    host=\"eu-west.cloud.hopsworks.ai\",\n",
    "    project=\"London_traffic\"\n",
    ")\n",
    "fs = project.get_feature_store()\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5aeb6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_timestamp_col(df: pd.DataFrame, candidates=RAW_TS_CANDIDATES) -> str | None:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def ensure_ts_10m(df: pd.DataFrame, ts_10m_col: str = TS_10M_COL) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    if ts_10m_col in df.columns:\n",
    "        df[ts_10m_col] = pd.to_datetime(df[ts_10m_col], utc=True, errors=\"coerce\")\n",
    "        return df\n",
    "\n",
    "    ts_col = find_timestamp_col(df)\n",
    "    if ts_col is None:\n",
    "        raise ValueError(\n",
    "            f\"No timestamp column found. Looked for {RAW_TS_CANDIDATES}. Available={list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\")\n",
    "    df[ts_10m_col] = df[ts_col].dt.floor(\"10min\")\n",
    "    return df\n",
    "\n",
    "def read_fg(fs, name: str, version: int) -> pd.DataFrame:\n",
    "    fg = fs.get_feature_group(name=name, version=version)\n",
    "    return fg.read()\n",
    "\n",
    "def join_two(left: pd.DataFrame, right: pd.DataFrame, right_name: str) -> pd.DataFrame:\n",
    "    left = left.copy()\n",
    "    right = right.copy()\n",
    "\n",
    "    if TS_10M_COL not in left.columns or TS_10M_COL not in right.columns:\n",
    "        raise ValueError(f\"Missing {TS_10M_COL} in join: left_has={TS_10M_COL in left.columns}, right_has={TS_10M_COL in right.columns}\")\n",
    "\n",
    "    left_has_pid = POINT_ID_COL in left.columns\n",
    "    right_has_pid = POINT_ID_COL in right.columns\n",
    "\n",
    "    if left_has_pid and right_has_pid:\n",
    "        keys = [POINT_ID_COL, TS_10M_COL]\n",
    "        out = left.merge(right, on=keys, how=\"left\", suffixes=(\"\", f\"_{right_name}\"))\n",
    "        print(f\"Joined {right_name} on {keys}. Shape={out.shape}\")\n",
    "        return out\n",
    "\n",
    "    keys = [TS_10M_COL]\n",
    "    out = left.merge(right, on=keys, how=\"left\", suffixes=(\"\", f\"_{right_name}\"))\n",
    "    print(f\"Joined {right_name} on {keys} only (no point_id in one side). Shape={out.shape}\")\n",
    "    return out\n",
    "\n",
    "def latest_per_point(df: pd.DataFrame, n_points: int = N_POINTS) -> pd.DataFrame:\n",
    "    if POINT_ID_COL not in df.columns:\n",
    "        raise ValueError(f\"{POINT_ID_COL} not found; cannot take latest per point.\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[TS_10M_COL])\n",
    "    df = df.sort_values([POINT_ID_COL, TS_10M_COL])\n",
    "    df = df.drop_duplicates(subset=[POINT_ID_COL], keep=\"last\")\n",
    "    df = df.sort_values(TS_10M_COL, ascending=False).head(n_points).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f67322ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.91s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (2.51s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (4.61s) \n",
      "Raw shapes:\n",
      "traffic: (26157, 33)\n",
      "weather: (230400, 6)\n",
      "tfl: (389982, 7)\n",
      "Has columns:\n",
      "traffic: ['timestamp_utc', 'point_id', 'frc', 'current_speed', 'free_flow_speed', 'current_travel_time', 'free_flow_travel_time', 'confidence', 'road_closure', 'ts_10m', 'speed_ratio', 'delay_seconds', 'day_of_week', 'is_weekend', 'hour', 'minute', 'is_rush_hour', 'ti_evening_peak', 'ti_midday', 'ti_morning_peak', 'ti_night', 'speed_diff', 'travel_time_ratio', 'low_confidence_flag', 'speed_roll_mean_3']\n",
      "weather: ['point_id', 'ts_10m', 'precipitation', 'rain', 'snowfall', 'temperature_2m']\n",
      "tfl: ['point_id', 'ts_10m', 'disruption_count', 'is_active', 'is_incident', 'is_works', 'max_ordinal']\n"
     ]
    }
   ],
   "source": [
    "df_tr = read_fg(fs, TRAFFIC_FG_NAME, TRAFFIC_FG_VER)\n",
    "df_we = read_fg(fs, WEATHER_FG_NAME, WEATHER_FG_VER)\n",
    "df_tf = read_fg(fs, TFL_FG_NAME, TFL_FG_VER)\n",
    "\n",
    "print(\"Raw shapes:\")\n",
    "print(\"traffic:\", df_tr.shape)\n",
    "print(\"weather:\", df_we.shape)\n",
    "print(\"tfl:\", df_tf.shape)\n",
    "\n",
    "df_tr = ensure_ts_10m(df_tr)\n",
    "df_we = ensure_ts_10m(df_we)\n",
    "df_tf = ensure_ts_10m(df_tf)\n",
    "\n",
    "print(\"Has columns:\")\n",
    "print(\"traffic:\", list(df_tr.columns)[:25])\n",
    "print(\"weather:\", list(df_we.columns)[:25])\n",
    "print(\"tfl:\", list(df_tf.columns)[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91477548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined weather on ['point_id', 'ts_10m']. Shape=(26157, 37)\n",
      "Joined tfl on ['point_id', 'ts_10m']. Shape=(26157, 42)\n",
      "Inference snapshot shape: (50, 42)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_id</th>\n",
       "      <th>ts_10m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36825</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16228</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26114</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38022</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6097</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26146</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26182</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36942</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26434</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26664</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  point_id                    ts_10m\n",
       "0    36825 2026-01-11 16:00:00+00:00\n",
       "1    16228 2026-01-11 16:00:00+00:00\n",
       "2    26114 2026-01-11 16:00:00+00:00\n",
       "3    38022 2026-01-11 16:00:00+00:00\n",
       "4     6097 2026-01-11 16:00:00+00:00\n",
       "5    26146 2026-01-11 16:00:00+00:00\n",
       "6    26182 2026-01-11 16:00:00+00:00\n",
       "7    36942 2026-01-11 16:00:00+00:00\n",
       "8    26434 2026-01-11 16:00:00+00:00\n",
       "9    26664 2026-01-11 16:00:00+00:00"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join = df_tr.copy()\n",
    "df_join = join_two(df_join, df_we, \"weather\")\n",
    "df_join = join_two(df_join, df_tf, \"tfl\")\n",
    "\n",
    "df_latest = latest_per_point(df_join, n_points=N_POINTS)\n",
    "\n",
    "print(\"Inference snapshot shape:\", df_latest.shape)\n",
    "df_latest[[POINT_ID_COL, TS_10M_COL]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d6809023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _walk_files(root_dir: str):\n",
    "    for r, _, files in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            yield os.path.join(r, f)\n",
    "\n",
    "def _find_saved_model_dir(root_dir: str):\n",
    "    for path in _walk_files(root_dir):\n",
    "        if os.path.basename(path) == \"saved_model.pb\":\n",
    "            return os.path.dirname(path)\n",
    "    return None\n",
    "\n",
    "def pick_model_from_registry(mr, model_name: str, model_version: int | None = None):\n",
    "    if model_version is not None:\n",
    "        model = mr.get_model(model_name, version=model_version)\n",
    "        print(f\"Using model {model_name} v{model_version}\")\n",
    "        return model\n",
    "\n",
    "    try:\n",
    "        model = mr.get_model(model_name, stage=\"production\")\n",
    "        print(f\"Using model {model_name} (stage=production) v{model.version}\")\n",
    "        return model\n",
    "    except Exception:\n",
    "        print(\"No production stage found. Falling back to latest version...\")\n",
    "        models = mr.get_models(model_name)\n",
    "        if len(models) == 0:\n",
    "            raise ValueError(f\"No models found with name={model_name}\")\n",
    "        latest = sorted(models, key=lambda m: m.version)[-1]\n",
    "        model = mr.get_model(model_name, version=latest.version)\n",
    "        print(f\"Using model {model_name} v{model.version} (latest)\")\n",
    "        return model\n",
    "\n",
    "def load_scaler_and_keras_from_dir(local_dir: str):\n",
    "    # 1) Find keras file\n",
    "    keras_file = None\n",
    "    for path in _walk_files(local_dir):\n",
    "        low = path.lower()\n",
    "        if low.endswith(\".keras\") or low.endswith(\".h5\") or low.endswith(\".hdf5\"):\n",
    "            keras_file = path\n",
    "            break\n",
    "\n",
    "    # 2) Find SavedModel directory\n",
    "    saved_model_dir = None\n",
    "    if keras_file is None:\n",
    "        saved_model_dir = _find_saved_model_dir(local_dir)\n",
    "\n",
    "    # 3) Find scaler-like pkl/joblib (object has transform but not predict)\n",
    "    scaler = None\n",
    "    pkl_candidates = [p for p in _walk_files(local_dir) if p.lower().endswith((\".pkl\", \".joblib\"))]\n",
    "    for p in pkl_candidates:\n",
    "        try:\n",
    "            obj = joblib.load(p)\n",
    "            if hasattr(obj, \"transform\") and hasattr(obj, \"fit\") and not hasattr(obj, \"predict\"):\n",
    "                scaler = obj\n",
    "                print(\"Loaded scaler artifact:\", p)\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 4) Load keras model\n",
    "    import tensorflow as tf\n",
    "    if keras_file is not None:\n",
    "        keras_model = tf.keras.models.load_model(keras_file)\n",
    "        print(\"Loaded Keras model file:\", keras_file)\n",
    "        return scaler, keras_model\n",
    "\n",
    "    if saved_model_dir is not None:\n",
    "        keras_model = tf.keras.models.load_model(saved_model_dir)\n",
    "        print(\"Loaded SavedModel directory:\", saved_model_dir)\n",
    "        return scaler, keras_model\n",
    "\n",
    "    # 5) Debug: print small tree\n",
    "    print(\"Could not find .keras/.h5 or SavedModel. Directory tree (limited):\")\n",
    "    for r, d, f in os.walk(local_dir):\n",
    "        depth = r.replace(local_dir, \"\").count(os.sep)\n",
    "        if depth > 3:\n",
    "            continue\n",
    "        print(\"  \" * depth + os.path.basename(r) + \"/\")\n",
    "        for ff in f[:20]:\n",
    "            print(\"  \" * (depth + 1) + ff)\n",
    "\n",
    "    raise FileNotFoundError(f\"No Keras model found under {local_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e73dd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No production stage found. Falling back to latest version...\n",
      "Using model traffic_speed_ratio_keras v1 (latest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100.000%|██████████| 1535/1535 elapsed<00:00 remaining<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 1 files)... \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100.000%|██████████| 697237/697237 elapsed<00:00 remaining<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 2 files)... \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100.000%|██████████| 312/312 elapsed<00:00 remaining<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 3 files)... \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100.000%|██████████| 180/180 elapsed<00:00 remaining<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 4 files)... \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100.000%|██████████| 956/956 elapsed<00:00 remaining<00:00\n",
      "InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to: /var/folders/bl/wz4tv3rs7vsd1fmkwrz2gnsc0000gn/T/d354cf28-bf49-4ce0-8b31-8749e753414a/traffic_speed_ratio_keras/1\n",
      "Top-level files: ['metrics.json', 'scaler.pkl', 'model.keras', 'feature_cols.json', 'meta.json']\n",
      "Loaded scaler artifact: /var/folders/bl/wz4tv3rs7vsd1fmkwrz2gnsc0000gn/T/d354cf28-bf49-4ce0-8b31-8749e753414a/traffic_speed_ratio_keras/1/scaler.pkl\n",
      "Loaded Keras model file: /var/folders/bl/wz4tv3rs7vsd1fmkwrz2gnsc0000gn/T/d354cf28-bf49-4ce0-8b31-8749e753414a/traffic_speed_ratio_keras/1/model.keras\n",
      "Scaler loaded: True\n",
      "Keras model loaded ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n"
     ]
    }
   ],
   "source": [
    "model_meta = pick_model_from_registry(mr, MODEL_NAME, MODEL_VERSION)\n",
    "local_dir = model_meta.download()\n",
    "print(\"Downloaded to:\", local_dir)\n",
    "\n",
    "# Optional: print top-level files to debug fast\n",
    "try:\n",
    "    print(\"Top-level files:\", os.listdir(local_dir))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "scaler, keras_model = load_scaler_and_keras_from_dir(local_dir)\n",
    "print(\"Scaler loaded:\", scaler is not None)\n",
    "print(\"Keras model loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5392f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (50, 39)\n",
      "Feature cols sample: ['frc', 'current_speed', 'free_flow_speed', 'current_travel_time', 'free_flow_travel_time', 'confidence', 'road_closure', 'speed_ratio', 'delay_seconds', 'day_of_week', 'is_weekend', 'hour', 'minute', 'is_rush_hour', 'ti_evening_peak', 'ti_midday', 'ti_morning_peak', 'ti_night', 'speed_diff', 'travel_time_ratio', 'low_confidence_flag', 'speed_roll_mean_3', 'speed_roll_std_3', 'delay_roll_mean_3', 'speed_roll_mean_6']\n"
     ]
    }
   ],
   "source": [
    "# Output columns to keep\n",
    "OUTPUT_COLS = [POINT_ID_COL, TS_10M_COL]\n",
    "OUTPUT_COLS = [c for c in OUTPUT_COLS if c in df_latest.columns]\n",
    "\n",
    "# Exclude typical non-features\n",
    "EXCLUDE_COLS = {\n",
    "    POINT_ID_COL,\n",
    "    TS_10M_COL,\n",
    "    \"timestamp\",\n",
    "    \"timestamp_utc\",\n",
    "    # possible labels\n",
    "    \"speed_ratio_t+30\", \"speed_ratio_t+60\",\n",
    "    \"label_t+30\", \"label_t+60\",\n",
    "}\n",
    "\n",
    "feature_cols = [c for c in df_latest.columns if c not in EXCLUDE_COLS]\n",
    "X = df_latest[feature_cols].copy()\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Feature cols sample:\", feature_cols[:25])\n",
    "\n",
    "# Convert bool -> int\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == bool:\n",
    "        X[c] = X[c].astype(int)\n",
    "\n",
    "# Convert object -> numeric if possible\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == \"object\":\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "\n",
    "# Fill NaNs (ideally match training; default safe)\n",
    "X = X.fillna(0.0)\n",
    "\n",
    "# Ensure float32 for keras\n",
    "X_values = X.values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d14f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keras_predict_two_horizons(model, X_np):\n",
    "    pred = model.predict(X_np, verbose=0)\n",
    "\n",
    "    if isinstance(pred, (list, tuple)) and len(pred) >= 2:\n",
    "        p30 = np.asarray(pred[0]).reshape(-1)\n",
    "        p60 = np.asarray(pred[1]).reshape(-1)\n",
    "        return p30, p60\n",
    "\n",
    "    pred = np.asarray(pred)\n",
    "    if pred.ndim == 2 and pred.shape[1] >= 2:\n",
    "        return pred[:, 0].reshape(-1), pred[:, 1].reshape(-1)\n",
    "\n",
    "    if pred.ndim == 1 or (pred.ndim == 2 and pred.shape[1] == 1):\n",
    "        return pred.reshape(-1), np.full((pred.shape[0],), np.nan)\n",
    "\n",
    "    raise ValueError(f\"Unexpected prediction shape: {pred.shape}\")\n",
    "\n",
    "# Scale if scaler exists\n",
    "if scaler is not None:\n",
    "    X_scaled = scaler.transform(X_values)\n",
    "    X_scaled = np.asarray(X_scaled).astype(np.float32)\n",
    "else:\n",
    "    X_scaled = X_values\n",
    "\n",
    "pred_30, pred_60 = keras_predict_two_horizons(keras_model, X_scaled)\n",
    "\n",
    "print(\"Pred shapes:\", pred_30.shape, pred_60.shape)\n",
    "print(\"Pred sample:\", pred_30[:5], pred_60[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016c3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_latest[OUTPUT_COLS].copy()\n",
    "df_pred[PRED_COL_30] = pred_30\n",
    "df_pred[PRED_COL_60] = pred_60\n",
    "\n",
    "df_pred[POINT_ID_COL] = df_pred[POINT_ID_COL].astype(str)\n",
    "df_pred = df_pred.sort_values(POINT_ID_COL).reset_index(drop=True)\n",
    "\n",
    "df_pred.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
