{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca483a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\omarl\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import hopsworks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20094c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Hopsworks\n",
    "# =========================\n",
    "PROJECT_NAME = os.environ.get(\"HOPSWORKS_PROJECT_NAME\")\n",
    "HOPSWORKS_API_KEY = os.environ.get(\"HOPSWORKS_API_KEY\")\n",
    "\n",
    "# =========================\n",
    "# Feature Groups\n",
    "# =========================\n",
    "TRAFFIC_FG_NAME, TRAFFIC_FG_VER = \"traffic_temporal_fg\", 1\n",
    "WEATHER_FG_NAME, WEATHER_FG_VER = \"weather_10m_fg\", 1\n",
    "TFL_FG_NAME, TFL_FG_VER         = \"tfl_disruptions_10m_fg\", 1\n",
    "\n",
    "# =========================\n",
    "# Columns\n",
    "# =========================\n",
    "POINT_ID_COL = \"point_id\"\n",
    "\n",
    "# If your FG already has ts_10m use that; otherwise we create it from timestamp_utc\n",
    "TS_10M_COL = \"ts_10m\"\n",
    "RAW_TS_CANDIDATES = [\"timestamp_utc\", \"timestamp\", \"datetime\", \"time\"]\n",
    "\n",
    "# =========================\n",
    "# Model\n",
    "# =========================\n",
    "MODEL_NAME = \"traffic_speed_ratio_keras\"   \n",
    "MODEL_VERSION = None                      \n",
    "\n",
    "PRED_COL_30 = \"pred_speed_ratio_t+30\"\n",
    "PRED_COL_60 = \"pred_speed_ratio_t+60\"\n",
    "\n",
    "# =========================\n",
    "# Output\n",
    "# =========================\n",
    "N_POINTS = 50\n",
    "OUT_JSON_PATH = \"data/predictions/predictions_latest.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9df09e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-11 11:38:17,103 INFO: Initializing external client\n",
      "2026-01-11 11:38:17,104 INFO: Base URL: https://eu-west.cloud.hopsworks.ai:443\n",
      "2026-01-11 11:38:17,942 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://eu-west.cloud.hopsworks.ai:443/p/3209\n"
     ]
    }
   ],
   "source": [
    "# 1. Login to Hopsworks\n",
    "project = hopsworks.login(\n",
    "    host=\"eu-west.cloud.hopsworks.ai\",\n",
    "    project=\"London_traffic\"\n",
    ")\n",
    "fs = project.get_feature_store()\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aeb6a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_timestamp_col(df: pd.DataFrame, candidates=RAW_TS_CANDIDATES) -> str | None:\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def ensure_ts_10m(df: pd.DataFrame, ts_10m_col: str = TS_10M_COL) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    if ts_10m_col in df.columns:\n",
    "        df[ts_10m_col] = pd.to_datetime(df[ts_10m_col], utc=True, errors=\"coerce\")\n",
    "        return df\n",
    "\n",
    "    ts_col = find_timestamp_col(df)\n",
    "    if ts_col is None:\n",
    "        raise ValueError(\n",
    "            f\"No timestamp column found. Looked for {RAW_TS_CANDIDATES}. Available={list(df.columns)}\"\n",
    "        )\n",
    "\n",
    "    df[ts_col] = pd.to_datetime(df[ts_col], utc=True, errors=\"coerce\")\n",
    "    df[ts_10m_col] = df[ts_col].dt.floor(\"10min\")\n",
    "    return df\n",
    "\n",
    "def read_fg(fs, name: str, version: int) -> pd.DataFrame:\n",
    "    fg = fs.get_feature_group(name=name, version=version)\n",
    "    return fg.read()\n",
    "\n",
    "def join_two(left: pd.DataFrame, right: pd.DataFrame, right_name: str) -> pd.DataFrame:\n",
    "    left = left.copy()\n",
    "    right = right.copy()\n",
    "\n",
    "    if TS_10M_COL not in left.columns or TS_10M_COL not in right.columns:\n",
    "        raise ValueError(f\"Missing {TS_10M_COL} in join: left_has={TS_10M_COL in left.columns}, right_has={TS_10M_COL in right.columns}\")\n",
    "\n",
    "    left_has_pid = POINT_ID_COL in left.columns\n",
    "    right_has_pid = POINT_ID_COL in right.columns\n",
    "\n",
    "    if left_has_pid and right_has_pid:\n",
    "        keys = [POINT_ID_COL, TS_10M_COL]\n",
    "        out = left.merge(right, on=keys, how=\"left\", suffixes=(\"\", f\"_{right_name}\"))\n",
    "        print(f\"Joined {right_name} on {keys}. Shape={out.shape}\")\n",
    "        return out\n",
    "\n",
    "    keys = [TS_10M_COL]\n",
    "    out = left.merge(right, on=keys, how=\"left\", suffixes=(\"\", f\"_{right_name}\"))\n",
    "    print(f\"Joined {right_name} on {keys} only (no point_id in one side). Shape={out.shape}\")\n",
    "    return out\n",
    "\n",
    "def latest_per_point(df: pd.DataFrame, n_points: int = N_POINTS) -> pd.DataFrame:\n",
    "    if POINT_ID_COL not in df.columns:\n",
    "        raise ValueError(f\"{POINT_ID_COL} not found; cannot take latest per point.\")\n",
    "\n",
    "    df = df.copy()\n",
    "    df = df.dropna(subset=[TS_10M_COL])\n",
    "    df = df.sort_values([POINT_ID_COL, TS_10M_COL])\n",
    "    df = df.drop_duplicates(subset=[POINT_ID_COL], keep=\"last\")\n",
    "    df = df.sort_values(TS_10M_COL, ascending=False).head(n_points).reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f67322ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (1.87s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (4.25s) \n",
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (5.07s) \n",
      "Raw shapes:\n",
      "traffic: (26157, 33)\n",
      "weather: (230400, 6)\n",
      "tfl: (389982, 7)\n",
      "Has columns:\n",
      "traffic: ['timestamp_utc', 'point_id', 'frc', 'current_speed', 'free_flow_speed', 'current_travel_time', 'free_flow_travel_time', 'confidence', 'road_closure', 'ts_10m', 'speed_ratio', 'delay_seconds', 'day_of_week', 'is_weekend', 'hour', 'minute', 'is_rush_hour', 'ti_evening_peak', 'ti_midday', 'ti_morning_peak', 'ti_night', 'speed_diff', 'travel_time_ratio', 'low_confidence_flag', 'speed_roll_mean_3']\n",
      "weather: ['point_id', 'ts_10m', 'precipitation', 'rain', 'snowfall', 'temperature_2m']\n",
      "tfl: ['point_id', 'ts_10m', 'disruption_count', 'is_active', 'is_incident', 'is_works', 'max_ordinal']\n"
     ]
    }
   ],
   "source": [
    "df_tr = read_fg(fs, TRAFFIC_FG_NAME, TRAFFIC_FG_VER)\n",
    "df_we = read_fg(fs, WEATHER_FG_NAME, WEATHER_FG_VER)\n",
    "df_tf = read_fg(fs, TFL_FG_NAME, TFL_FG_VER)\n",
    "\n",
    "print(\"Raw shapes:\")\n",
    "print(\"traffic:\", df_tr.shape)\n",
    "print(\"weather:\", df_we.shape)\n",
    "print(\"tfl:\", df_tf.shape)\n",
    "\n",
    "df_tr = ensure_ts_10m(df_tr)\n",
    "df_we = ensure_ts_10m(df_we)\n",
    "df_tf = ensure_ts_10m(df_tf)\n",
    "\n",
    "print(\"Has columns:\")\n",
    "print(\"traffic:\", list(df_tr.columns)[:25])\n",
    "print(\"weather:\", list(df_we.columns)[:25])\n",
    "print(\"tfl:\", list(df_tf.columns)[:25])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91477548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined weather on ['point_id', 'ts_10m']. Shape=(26157, 37)\n",
      "Joined tfl on ['point_id', 'ts_10m']. Shape=(26157, 42)\n",
      "Inference snapshot shape: (50, 42)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_id</th>\n",
       "      <th>ts_10m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36825</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16228</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26114</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38022</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6097</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>26146</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>26182</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>36942</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26434</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26664</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  point_id                    ts_10m\n",
       "0    36825 2026-01-11 16:00:00+00:00\n",
       "1    16228 2026-01-11 16:00:00+00:00\n",
       "2    26114 2026-01-11 16:00:00+00:00\n",
       "3    38022 2026-01-11 16:00:00+00:00\n",
       "4     6097 2026-01-11 16:00:00+00:00\n",
       "5    26146 2026-01-11 16:00:00+00:00\n",
       "6    26182 2026-01-11 16:00:00+00:00\n",
       "7    36942 2026-01-11 16:00:00+00:00\n",
       "8    26434 2026-01-11 16:00:00+00:00\n",
       "9    26664 2026-01-11 16:00:00+00:00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_join = df_tr.copy()\n",
    "df_join = join_two(df_join, df_we, \"weather\")\n",
    "df_join = join_two(df_join, df_tf, \"tfl\")\n",
    "\n",
    "df_latest = latest_per_point(df_join, n_points=N_POINTS)\n",
    "\n",
    "print(\"Inference snapshot shape:\", df_latest.shape)\n",
    "df_latest[[POINT_ID_COL, TS_10M_COL]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6809023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _walk_files(root_dir: str):\n",
    "    for r, _, files in os.walk(root_dir):\n",
    "        for f in files:\n",
    "            yield os.path.join(r, f)\n",
    "\n",
    "def _find_saved_model_dir(root_dir: str):\n",
    "    for path in _walk_files(root_dir):\n",
    "        if os.path.basename(path) == \"saved_model.pb\":\n",
    "            return os.path.dirname(path)\n",
    "    return None\n",
    "\n",
    "def pick_model_from_registry(mr, model_name: str, model_version: int | None = None):\n",
    "    if model_version is not None:\n",
    "        model = mr.get_model(model_name, version=model_version)\n",
    "        print(f\"Using model {model_name} v{model_version}\")\n",
    "        return model\n",
    "\n",
    "    try:\n",
    "        model = mr.get_model(model_name, stage=\"production\")\n",
    "        print(f\"Using model {model_name} (stage=production) v{model.version}\")\n",
    "        return model\n",
    "    except Exception:\n",
    "        print(\"No production stage found. Falling back to latest version...\")\n",
    "        models = mr.get_models(model_name)\n",
    "        if len(models) == 0:\n",
    "            raise ValueError(f\"No models found with name={model_name}\")\n",
    "        latest = sorted(models, key=lambda m: m.version)[-1]\n",
    "        model = mr.get_model(model_name, version=latest.version)\n",
    "        print(f\"Using model {model_name} v{model.version} (latest)\")\n",
    "        return model\n",
    "\n",
    "def load_scaler_and_keras_from_dir(local_dir: str):\n",
    "    # 1) Find keras file\n",
    "    keras_file = None\n",
    "    for path in _walk_files(local_dir):\n",
    "        low = path.lower()\n",
    "        if low.endswith(\".keras\") or low.endswith(\".h5\") or low.endswith(\".hdf5\"):\n",
    "            keras_file = path\n",
    "            break\n",
    "\n",
    "    # 2) Find SavedModel directory\n",
    "    saved_model_dir = None\n",
    "    if keras_file is None:\n",
    "        saved_model_dir = _find_saved_model_dir(local_dir)\n",
    "\n",
    "    # 3) Find scaler-like pkl/joblib (object has transform but not predict)\n",
    "    scaler = None\n",
    "    pkl_candidates = [p for p in _walk_files(local_dir) if p.lower().endswith((\".pkl\", \".joblib\"))]\n",
    "    for p in pkl_candidates:\n",
    "        try:\n",
    "            obj = joblib.load(p)\n",
    "            if hasattr(obj, \"transform\") and hasattr(obj, \"fit\") and not hasattr(obj, \"predict\"):\n",
    "                scaler = obj\n",
    "                print(\"Loaded scaler artifact:\", p)\n",
    "                break\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 4) Load keras model\n",
    "    import tensorflow as tf\n",
    "    if keras_file is not None:\n",
    "        keras_model = tf.keras.models.load_model(keras_file)\n",
    "        print(\"Loaded Keras model file:\", keras_file)\n",
    "        return scaler, keras_model\n",
    "\n",
    "    if saved_model_dir is not None:\n",
    "        keras_model = tf.keras.models.load_model(saved_model_dir)\n",
    "        print(\"Loaded SavedModel directory:\", saved_model_dir)\n",
    "        return scaler, keras_model\n",
    "\n",
    "    # 5) Debug: print small tree\n",
    "    print(\"Could not find .keras/.h5 or SavedModel. Directory tree (limited):\")\n",
    "    for r, d, f in os.walk(local_dir):\n",
    "        depth = r.replace(local_dir, \"\").count(os.sep)\n",
    "        if depth > 3:\n",
    "            continue\n",
    "        print(\"  \" * depth + os.path.basename(r) + \"/\")\n",
    "        for ff in f[:20]:\n",
    "            print(\"  \" * (depth + 1) + ff)\n",
    "\n",
    "    raise FileNotFoundError(f\"No Keras model found under {local_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e73dd6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No production stage found. Falling back to latest version...\n",
      "Using model traffic_speed_ratio_keras v1 (latest)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100.000%|██████████| 1535/1535 elapsed<00:00 remaining<?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 1 files)... \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100.000%|██████████| 697237/697237 elapsed<00:00 remaining<00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 2 files)... \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100.000%|██████████| 312/312 elapsed<00:00 remaining<?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 3 files)... \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100.000%|██████████| 180/180 elapsed<00:00 remaining<?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model artifact (0 dirs, 4 files)... \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100.000%|██████████| 956/956 elapsed<00:00 remaining<?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded to: C:\\Users\\omarl\\AppData\\Local\\Temp\\6f31a82b-6d7f-4348-9c1a-98166ed7b248\\traffic_speed_ratio_keras/1\n",
      "Top-level files: ['feature_cols.json', 'meta.json', 'metrics.json', 'model.keras', 'scaler.pkl']\n",
      "Loaded scaler artifact: C:\\Users\\omarl\\AppData\\Local\\Temp\\6f31a82b-6d7f-4348-9c1a-98166ed7b248\\traffic_speed_ratio_keras/1\\scaler.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Keras model file: C:\\Users\\omarl\\AppData\\Local\\Temp\\6f31a82b-6d7f-4348-9c1a-98166ed7b248\\traffic_speed_ratio_keras/1\\model.keras\n",
      "Scaler loaded: True\n",
      "Keras model loaded ✅\n"
     ]
    }
   ],
   "source": [
    "model_meta = pick_model_from_registry(mr, MODEL_NAME, MODEL_VERSION)\n",
    "local_dir = model_meta.download()\n",
    "print(\"Downloaded to:\", local_dir)\n",
    "\n",
    "# Optional: print top-level files to debug fast\n",
    "try:\n",
    "    print(\"Top-level files:\", os.listdir(local_dir))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "scaler, keras_model = load_scaler_and_keras_from_dir(local_dir)\n",
    "print(\"Scaler loaded:\", scaler is not None)\n",
    "print(\"Keras model loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5392f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (50, 39)\n",
      "Feature cols sample: ['frc', 'current_speed', 'free_flow_speed', 'current_travel_time', 'free_flow_travel_time', 'confidence', 'road_closure', 'speed_ratio', 'delay_seconds', 'day_of_week', 'is_weekend', 'hour', 'minute', 'is_rush_hour', 'ti_evening_peak', 'ti_midday', 'ti_morning_peak', 'ti_night', 'speed_diff', 'travel_time_ratio', 'low_confidence_flag', 'speed_roll_mean_3', 'speed_roll_std_3', 'delay_roll_mean_3', 'speed_roll_mean_6']\n"
     ]
    }
   ],
   "source": [
    "# Output columns to keep\n",
    "OUTPUT_COLS = [POINT_ID_COL, TS_10M_COL]\n",
    "OUTPUT_COLS = [c for c in OUTPUT_COLS if c in df_latest.columns]\n",
    "\n",
    "# Exclude typical non-features\n",
    "EXCLUDE_COLS = {\n",
    "    POINT_ID_COL,\n",
    "    TS_10M_COL,\n",
    "    \"timestamp\",\n",
    "    \"timestamp_utc\",\n",
    "    # possible labels\n",
    "    \"speed_ratio_t+30\", \"speed_ratio_t+60\",\n",
    "    \"label_t+30\", \"label_t+60\",\n",
    "}\n",
    "\n",
    "feature_cols = [c for c in df_latest.columns if c not in EXCLUDE_COLS]\n",
    "X = df_latest[feature_cols].copy()\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"Feature cols sample:\", feature_cols[:25])\n",
    "\n",
    "# Convert bool -> int\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == bool:\n",
    "        X[c] = X[c].astype(int)\n",
    "\n",
    "# Convert object -> numeric if possible\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == \"object\":\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "\n",
    "# Fill NaNs (ideally match training; default safe)\n",
    "X = X.fillna(0.0)\n",
    "\n",
    "# Ensure float32 for keras\n",
    "X_values = X.values.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8d14f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred shapes: (50,) (50,)\n",
      "Pred sample: [0.89868975 0.8712174  0.73504764 0.84368205 0.8861835 ] [0.90779924 0.8620744  0.7338985  0.8298802  0.8843186 ]\n"
     ]
    }
   ],
   "source": [
    "def keras_predict_two_horizons(model, X_np):\n",
    "    pred = model.predict(X_np, verbose=0)\n",
    "\n",
    "    if isinstance(pred, (list, tuple)) and len(pred) >= 2:\n",
    "        p30 = np.asarray(pred[0]).reshape(-1)\n",
    "        p60 = np.asarray(pred[1]).reshape(-1)\n",
    "        return p30, p60\n",
    "\n",
    "    pred = np.asarray(pred)\n",
    "    if pred.ndim == 2 and pred.shape[1] >= 2:\n",
    "        return pred[:, 0].reshape(-1), pred[:, 1].reshape(-1)\n",
    "\n",
    "    if pred.ndim == 1 or (pred.ndim == 2 and pred.shape[1] == 1):\n",
    "        return pred.reshape(-1), np.full((pred.shape[0],), np.nan)\n",
    "\n",
    "    raise ValueError(f\"Unexpected prediction shape: {pred.shape}\")\n",
    "\n",
    "# Scale if scaler exists\n",
    "if scaler is not None:\n",
    "    X_scaled = scaler.transform(X_values)\n",
    "    X_scaled = np.asarray(X_scaled).astype(np.float32)\n",
    "else:\n",
    "    X_scaled = X_values\n",
    "\n",
    "pred_30, pred_60 = keras_predict_two_horizons(keras_model, X_scaled)\n",
    "\n",
    "print(\"Pred shapes:\", pred_30.shape, pred_60.shape)\n",
    "print(\"Pred sample:\", pred_30[:5], pred_60[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2016c3a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_id</th>\n",
       "      <th>ts_10m</th>\n",
       "      <th>pred_speed_ratio_t+30</th>\n",
       "      <th>pred_speed_ratio_t+60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16228</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "      <td>0.871217</td>\n",
       "      <td>0.862074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16250</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "      <td>0.864029</td>\n",
       "      <td>0.875404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16435</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "      <td>0.861034</td>\n",
       "      <td>0.857013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16737</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "      <td>0.900048</td>\n",
       "      <td>0.901368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16741</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "      <td>0.823388</td>\n",
       "      <td>0.836902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16755</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "      <td>0.778052</td>\n",
       "      <td>0.762026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16756</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "      <td>0.654570</td>\n",
       "      <td>0.601123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16764</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "      <td>0.605961</td>\n",
       "      <td>0.533973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16791</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "      <td>0.864782</td>\n",
       "      <td>0.864342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17169</td>\n",
       "      <td>2026-01-11 16:00:00+00:00</td>\n",
       "      <td>0.687260</td>\n",
       "      <td>0.652561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  point_id                    ts_10m  pred_speed_ratio_t+30  \\\n",
       "0    16228 2026-01-11 16:00:00+00:00               0.871217   \n",
       "1    16250 2026-01-11 16:00:00+00:00               0.864029   \n",
       "2    16435 2026-01-11 16:00:00+00:00               0.861034   \n",
       "3    16737 2026-01-11 16:00:00+00:00               0.900048   \n",
       "4    16741 2026-01-11 16:00:00+00:00               0.823388   \n",
       "5    16755 2026-01-11 16:00:00+00:00               0.778052   \n",
       "6    16756 2026-01-11 16:00:00+00:00               0.654570   \n",
       "7    16764 2026-01-11 16:00:00+00:00               0.605961   \n",
       "8    16791 2026-01-11 16:00:00+00:00               0.864782   \n",
       "9    17169 2026-01-11 16:00:00+00:00               0.687260   \n",
       "\n",
       "   pred_speed_ratio_t+60  \n",
       "0               0.862074  \n",
       "1               0.875404  \n",
       "2               0.857013  \n",
       "3               0.901368  \n",
       "4               0.836902  \n",
       "5               0.762026  \n",
       "6               0.601123  \n",
       "7               0.533973  \n",
       "8               0.864342  \n",
       "9               0.652561  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pred = df_latest[OUTPUT_COLS].copy()\n",
    "df_pred[PRED_COL_30] = pred_30\n",
    "df_pred[PRED_COL_60] = pred_60\n",
    "\n",
    "df_pred[POINT_ID_COL] = df_pred[POINT_ID_COL].astype(str)\n",
    "df_pred = df_pred.sort_values(POINT_ID_COL).reset_index(drop=True)\n",
    "\n",
    "df_pred.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae95abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished: Reading data from Hopsworks, using Hopsworks Feature Query Service (2.75s) \n",
      "df_pred shape: (50, 4)\n",
      "df_meta shape: (200, 9)\n",
      "df_merged shape (should be 50 rows): (50, 12)\n",
      "df_ui shape: (50, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>point_id</th>\n",
       "      <th>ts_10m</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>road_name</th>\n",
       "      <th>road_category</th>\n",
       "      <th>road_type</th>\n",
       "      <th>local_authority_id</th>\n",
       "      <th>link_length_km</th>\n",
       "      <th>pred_speed_ratio_t+30</th>\n",
       "      <th>pred_speed_ratio_t+60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16228</td>\n",
       "      <td>2026-01-11T16:00:00Z</td>\n",
       "      <td>51.428910</td>\n",
       "      <td>0.084388</td>\n",
       "      <td>A20</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>106</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.871217</td>\n",
       "      <td>0.862074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16250</td>\n",
       "      <td>2026-01-11T16:00:00Z</td>\n",
       "      <td>51.497434</td>\n",
       "      <td>-0.111885</td>\n",
       "      <td>A23</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>107</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.864029</td>\n",
       "      <td>0.875404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16435</td>\n",
       "      <td>2026-01-11T16:00:00Z</td>\n",
       "      <td>51.522479</td>\n",
       "      <td>-0.160005</td>\n",
       "      <td>A4380</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>109</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.861034</td>\n",
       "      <td>0.857013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16737</td>\n",
       "      <td>2026-01-11T16:00:00Z</td>\n",
       "      <td>51.504060</td>\n",
       "      <td>-0.104551</td>\n",
       "      <td>A201</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>103</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.900048</td>\n",
       "      <td>0.901368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16741</td>\n",
       "      <td>2026-01-11T16:00:00Z</td>\n",
       "      <td>51.482807</td>\n",
       "      <td>0.062360</td>\n",
       "      <td>A205</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>105</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.823388</td>\n",
       "      <td>0.836902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16755</td>\n",
       "      <td>2026-01-11T16:00:00Z</td>\n",
       "      <td>51.421815</td>\n",
       "      <td>-0.052147</td>\n",
       "      <td>A213</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>176</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.778052</td>\n",
       "      <td>0.762026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16756</td>\n",
       "      <td>2026-01-11T16:00:00Z</td>\n",
       "      <td>51.383489</td>\n",
       "      <td>-0.105944</td>\n",
       "      <td>A213</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>134</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.654570</td>\n",
       "      <td>0.601123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16764</td>\n",
       "      <td>2026-01-11T16:00:00Z</td>\n",
       "      <td>51.463984</td>\n",
       "      <td>-0.215483</td>\n",
       "      <td>A219</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>108</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.605961</td>\n",
       "      <td>0.533973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16791</td>\n",
       "      <td>2026-01-11T16:00:00Z</td>\n",
       "      <td>51.389242</td>\n",
       "      <td>-0.305492</td>\n",
       "      <td>A243</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>178</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.864782</td>\n",
       "      <td>0.864342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17169</td>\n",
       "      <td>2026-01-11T16:00:00Z</td>\n",
       "      <td>51.526551</td>\n",
       "      <td>-0.133170</td>\n",
       "      <td>A501</td>\n",
       "      <td>PA</td>\n",
       "      <td>Major</td>\n",
       "      <td>145</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.687260</td>\n",
       "      <td>0.652561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  point_id                ts_10m   latitude  longitude road_name  \\\n",
       "0    16228  2026-01-11T16:00:00Z  51.428910   0.084388       A20   \n",
       "1    16250  2026-01-11T16:00:00Z  51.497434  -0.111885       A23   \n",
       "2    16435  2026-01-11T16:00:00Z  51.522479  -0.160005     A4380   \n",
       "3    16737  2026-01-11T16:00:00Z  51.504060  -0.104551      A201   \n",
       "4    16741  2026-01-11T16:00:00Z  51.482807   0.062360      A205   \n",
       "5    16755  2026-01-11T16:00:00Z  51.421815  -0.052147      A213   \n",
       "6    16756  2026-01-11T16:00:00Z  51.383489  -0.105944      A213   \n",
       "7    16764  2026-01-11T16:00:00Z  51.463984  -0.215483      A219   \n",
       "8    16791  2026-01-11T16:00:00Z  51.389242  -0.305492      A243   \n",
       "9    17169  2026-01-11T16:00:00Z  51.526551  -0.133170      A501   \n",
       "\n",
       "  road_category road_type  local_authority_id  link_length_km  \\\n",
       "0            PA     Major                 106             2.3   \n",
       "1            PA     Major                 107             0.3   \n",
       "2            PA     Major                 109             0.5   \n",
       "3            PA     Major                 103             0.9   \n",
       "4            PA     Major                 105             1.9   \n",
       "5            PA     Major                 176             1.3   \n",
       "6            PA     Major                 134             0.3   \n",
       "7            PA     Major                 108             0.5   \n",
       "8            PA     Major                 178             1.2   \n",
       "9            PA     Major                 145             0.4   \n",
       "\n",
       "   pred_speed_ratio_t+30  pred_speed_ratio_t+60  \n",
       "0               0.871217               0.862074  \n",
       "1               0.864029               0.875404  \n",
       "2               0.861034               0.857013  \n",
       "3               0.900048               0.901368  \n",
       "4               0.823388               0.836902  \n",
       "5               0.778052               0.762026  \n",
       "6               0.654570               0.601123  \n",
       "7               0.605961               0.533973  \n",
       "8               0.864782               0.864342  \n",
       "9               0.687260               0.652561  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# Merge ONLY the 50 predictions with metadata (200 -> 50 via left join)\n",
    "# =========================\n",
    "\n",
    "META_FG_NAME = \"traffic_points_metadata\"\n",
    "META_FG_VER = 1\n",
    "\n",
    "# 1) Read metadata (200)\n",
    "meta_fg = fs.get_feature_group(name=META_FG_NAME, version=META_FG_VER)\n",
    "df_meta = meta_fg.read()\n",
    "\n",
    "# 2) Ensure join key type\n",
    "df_pred = df_pred.copy()\n",
    "df_pred[POINT_ID_COL] = df_pred[POINT_ID_COL].astype(str)\n",
    "\n",
    "df_meta = df_meta.copy()\n",
    "df_meta[POINT_ID_COL] = df_meta[POINT_ID_COL].astype(str)\n",
    "\n",
    "# 3) If metadata has duplicates per point_id, keep last\n",
    "df_meta = df_meta.drop_duplicates(subset=[POINT_ID_COL], keep=\"last\")\n",
    "\n",
    "# 4) Left join: base is df_pred (50) => output stays 50\n",
    "df_merged = df_pred.merge(df_meta, on=POINT_ID_COL, how=\"left\", suffixes=(\"\", \"_meta\"))\n",
    "\n",
    "print(\"df_pred shape:\", df_pred.shape)\n",
    "print(\"df_meta shape:\", df_meta.shape)\n",
    "print(\"df_merged shape (should be 50 rows):\", df_merged.shape)\n",
    "\n",
    "# =========================\n",
    "# Build a CLEAN dataframe for UI\n",
    "# =========================\n",
    "\n",
    "# Columns we want (only keep those that exist)\n",
    "ui_cols_order = [\n",
    "    # IDs and time\n",
    "    POINT_ID_COL, TS_10M_COL,\n",
    "\n",
    "    # location / metadata (if present)\n",
    "    \"latitude\", \"longitude\",\n",
    "    \"road_name\", \"road_category\", \"road_type\",\n",
    "    \"region_id\", \"local_authority_id\",\n",
    "    \"start_junction_road_name\", \"end_junction_road_name\",\n",
    "    \"link_length_km\",\n",
    "    \"geometry\",\n",
    "\n",
    "    # current traffic state (if present)\n",
    "    \"frc\",\n",
    "    \"current_speed\", \"free_flow_speed\",\n",
    "    \"speed_ratio\", \"delay_seconds\",\n",
    "    \"current_travel_time\", \"free_flow_travel_time\",\n",
    "    \"travel_time_ratio\", \"speed_diff\",\n",
    "    \"confidence\", \"road_closure\",\n",
    "    \"low_confidence_flag\",\n",
    "    \"is_rush_hour\", \"is_weekend\",\n",
    "    \"day_of_week\", \"hour\", \"minute\",\n",
    "\n",
    "    # predictions\n",
    "    PRED_COL_30, PRED_COL_60\n",
    "]\n",
    "\n",
    "ui_cols = [c for c in ui_cols_order if c in df_merged.columns]\n",
    "df_ui = df_merged[ui_cols].copy()\n",
    "\n",
    "# =========================\n",
    "# Final cleanup for UI/JSON\n",
    "# =========================\n",
    "\n",
    "# Format ts_10m ISO\n",
    "if TS_10M_COL in df_ui.columns:\n",
    "    df_ui[TS_10M_COL] = pd.to_datetime(df_ui[TS_10M_COL], utc=True, errors=\"coerce\")\n",
    "    df_ui[TS_10M_COL] = df_ui[TS_10M_COL].dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# Convert bool -> int for UI friendliness\n",
    "for c in df_ui.columns:\n",
    "    if df_ui[c].dtype == bool:\n",
    "        df_ui[c] = df_ui[c].astype(int)\n",
    "\n",
    "# Numeric cleanup for important numeric columns (safe)\n",
    "numeric_cols = [\n",
    "    \"latitude\", \"longitude\",\n",
    "    \"current_speed\", \"free_flow_speed\", \"speed_ratio\", \"delay_seconds\",\n",
    "    \"confidence\", \"current_travel_time\", \"free_flow_travel_time\",\n",
    "    \"travel_time_ratio\", \"speed_diff\",\n",
    "    PRED_COL_30, PRED_COL_60\n",
    "]\n",
    "for c in numeric_cols:\n",
    "    if c in df_ui.columns:\n",
    "        df_ui[c] = pd.to_numeric(df_ui[c], errors=\"coerce\")\n",
    "\n",
    "# Keep order stable\n",
    "df_ui[POINT_ID_COL] = df_ui[POINT_ID_COL].astype(str)\n",
    "df_ui = df_ui.sort_values(POINT_ID_COL).reset_index(drop=True)\n",
    "\n",
    "print(\"df_ui shape:\", df_ui.shape)\n",
    "df_ui.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9d46fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions_latest.json \n",
      "{\"generated_at_utc\": \"2026-01-11T17:38:47Z\", \"model_name\": \"traffic_speed_ratio_keras\", \"model_version\": 1, \"n_points\": 50, \"predictions\": [{\"point_id\": \"16228\", \"ts_10m\": \"2026-01-11T16:00:00Z\", \"latitude\": 51.4289098, \"longitude\": 0.08438768, \"road_name\": \"A20\", \"road_category\": \"PA\", \"road_type\": \"Major\", \"local_authority_id\": 106, \"link_length_km\": 2.3, \"pred_speed_ratio_t+30\": 0.8712174296379089, \"pred_speed_ratio_t+60\": 0.8620743751525879}, {\"point_id\": \"16250\", \"ts_10m\": \"2026-01-11T16:00:00Z\", \"latitude\": 51.49743368, \"longitude\": -0.11188523, \"road_name\": \"A23\", \"road_category\": \"PA\", ...\n"
     ]
    }
   ],
   "source": [
    "payload = {\n",
    "    \"generated_at_utc\": datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"model_version\": getattr(model_meta, \"version\", None),\n",
    "    \"n_points\": int(len(df_ui)),\n",
    "    \"predictions\": df_ui.to_dict(orient=\"records\"),\n",
    "}\n",
    "\n",
    "json_str = json.dumps(payload, ensure_ascii=False)\n",
    "\n",
    "with open(\"predictions_latest.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json_str)\n",
    "\n",
    "print(\"Saved predictions_latest.json \")\n",
    "print(json_str[:600], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea17c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Prepare df_ui for Feature Group\n",
    "# =========================\n",
    "\n",
    "df_fg = df_ui.copy()\n",
    "\n",
    "# Asegura tipos correctos\n",
    "df_fg[POINT_ID_COL] = df_fg[POINT_ID_COL].astype(str)\n",
    "\n",
    "# event time como datetime (no string) para FG\n",
    "if TS_10M_COL in df_fg.columns:\n",
    "    df_fg[TS_10M_COL] = pd.to_datetime(df_fg[TS_10M_COL], utc=True, errors=\"coerce\")\n",
    "\n",
    "# bool -> int ya lo hicimos, ok\n",
    "# Asegura floats en preds\n",
    "for c in [PRED_COL_30, PRED_COL_60]:\n",
    "    if c in df_fg.columns:\n",
    "        df_fg[c] = pd.to_numeric(df_fg[c], errors=\"coerce\").astype(float)\n",
    "\n",
    "print(df_fg.dtypes)\n",
    "df_fg.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_FG_NAME = \"traffic_predictions_fg\"\n",
    "PRED_FG_VER = 1\n",
    "\n",
    "# intenta obtenerlo; si no existe, créalo\n",
    "try:\n",
    "    pred_fg = fs.get_feature_group(name=PRED_FG_NAME, version=PRED_FG_VER)\n",
    "    print(\"Prediction FG already exists ✅\")\n",
    "except Exception:\n",
    "    pred_fg = fs.create_feature_group(\n",
    "        name=PRED_FG_NAME,\n",
    "        version=PRED_FG_VER,\n",
    "        description=\"Traffic predictions for +30 and +60 minutes (joined with metadata, ready for UI).\",\n",
    "        primary_key=[POINT_ID_COL, TS_10M_COL],   # histórico: guarda cada timestamp\n",
    "        event_time=TS_10M_COL,\n",
    "        online_enabled=True\n",
    "    )\n",
    "    print(\"Created Prediction FG ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513dd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append every run (keeps history)\n",
    "pred_fg.insert(df_fg)\n",
    "\n",
    "print(\"Inserted predictions to Feature Group ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
